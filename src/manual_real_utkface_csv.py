#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰‹åŠ¨çœŸå®UTKFaceæ•°æ®CSVç”Ÿæˆå™¨
éœ€è¦ç”¨æˆ·æ‰‹åŠ¨ä¸‹è½½çœŸå®UTKFaceæ•°æ®é›†ï¼Œ100%ä½¿ç”¨çœŸå®æ•°æ®
æ ¼å¼ï¼šç‰¹å¾åˆ— | é¢„æµ‹å€¼ | çœŸå®å€¼ | ç»å¯¹è¯¯å·®
"""

import os
import sys
import numpy as np
import pandas as pd
import torch
import torchvision.transforms as transforms
from PIL import Image
import glob
from pathlib import Path
from typing import Tuple, List, Optional
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

class ManualUTKFaceDataChecker:
    """æ‰‹åŠ¨UTKFaceæ•°æ®æ£€æŸ¥å™¨"""
    
    def __init__(self, data_dir: str = "data"):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)
        
    def show_download_instructions(self):
        """æ˜¾ç¤ºè¯¦ç»†ä¸‹è½½æŒ‡å¯¼"""
        print("ğŸ¯ çœŸå®UTKFaceæ•°æ®é›†æ‰‹åŠ¨ä¸‹è½½æŒ‡å—")
        print("=" * 70)
        print("ğŸ“ ç”±äºUTKFaceæ•°æ®é›†çš„ç‰ˆæƒä¿æŠ¤ï¼Œéœ€è¦æ‚¨æ‰‹åŠ¨ä¸‹è½½çœŸå®æ•°æ®ï¼š")
        print()
        print("ğŸ”— ä¸‹è½½æ­¥éª¤ï¼š")
        print("   1. è®¿é—®Kaggle: https://www.kaggle.com/datasets/jangedoo/utkface-new")
        print("   2. æ³¨å†Œ/ç™»å½•Kaggleè´¦æˆ·")
        print("   3. ç‚¹å‡» 'Download' æŒ‰é’®ä¸‹è½½æ•°æ®é›†")
        print("   4. è§£å‹ä¸‹è½½çš„zipæ–‡ä»¶")
        print("   5. å°†æ‰€æœ‰.jpgå›¾åƒæ–‡ä»¶å¤åˆ¶åˆ°ä»¥ä¸‹ç›®å½•ï¼š")
        print(f"      {self.data_dir.absolute()}/")
        print()
        print("ğŸ“‹ æ–‡ä»¶æ ¼å¼è¦æ±‚ï¼š")
        print("   - æ–‡ä»¶åæ ¼å¼ï¼š[age]_[gender]_[race]_[timestamp].jpg")
        print("   - ç¤ºä¾‹ï¼š21_0_1_20170109142408075.jpg (21å²,å¥³æ€§,ç™½äºº)")
        print("   - æœ€å°‘éœ€è¦ï¼š100ä¸ªæœ‰æ•ˆå›¾åƒæ–‡ä»¶")
        print()
        print("ğŸ” æ›¿ä»£ä¸‹è½½æºï¼š")
        print("   - å®˜æ–¹ç½‘ç«™: https://susanqq.github.io/UTKFace/")
        print("   - GitHubé¡¹ç›®: https://github.com/aicip/UTKFace")
        print()
        print("âš ï¸  æ³¨æ„äº‹é¡¹ï¼š")
        print("   - è¯·ç¡®ä¿ä¸‹è½½çš„æ˜¯å®Œæ•´çš„UTKFaceæ•°æ®é›†")
        print("   - ä¸è¦ä¿®æ”¹åŸå§‹æ–‡ä»¶å")
        print("   - å›¾åƒæ–‡ä»¶åº”è¯¥æ˜¯RGBæ ¼å¼çš„äººè„¸å›¾åƒ")
        print("=" * 70)
    
    def check_real_data(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰çœŸå®UTKFaceæ•°æ®"""
        print("ğŸ” æ£€æŸ¥çœŸå®UTKFaceæ•°æ®...")
        
        # æœç´¢æ‰€æœ‰å¯èƒ½çš„å›¾åƒæ–‡ä»¶
        search_patterns = [
            self.data_dir / "*.jpg",
            self.data_dir / "*.jpeg", 
            self.data_dir / "**/*.jpg",
            self.data_dir / "**/*.jpeg",
            self.data_dir / "UTKFace" / "*.jpg",
            self.data_dir / "utkface-new" / "*.jpg",
            self.data_dir / "crop_part1" / "*.jpg",
        ]
        
        all_images = []
        for pattern in search_patterns:
            try:
                images = list(Path().glob(str(pattern)))
                all_images.extend(images)
            except:
                continue
        
        # å»é‡
        all_images = list(set(all_images))
        
        # éªŒè¯æ˜¯å¦ä¸ºçœŸå®UTKFaceæ ¼å¼çš„æ–‡ä»¶
        valid_images = []
        for img_path in all_images:
            if self.is_valid_utkface_file(img_path):
                valid_images.append(img_path)
        
        print(f"   ğŸ“Š æœç´¢ç»“æœï¼š")
        print(f"   æ€»å›¾åƒæ–‡ä»¶: {len(all_images)}")
        print(f"   æœ‰æ•ˆUTKFaceæ–‡ä»¶: {len(valid_images)}")
        
        if len(valid_images) >= 100:
            print(f"   âœ… æ‰¾åˆ°è¶³å¤Ÿçš„çœŸå®UTKFaceæ•°æ® ({len(valid_images)} ä¸ªæ–‡ä»¶)")
            
            # æ˜¾ç¤ºä¸€äº›ç¤ºä¾‹æ–‡ä»¶
            print(f"   ğŸ“‹ ç¤ºä¾‹æ–‡ä»¶:")
            for i, img_path in enumerate(valid_images[:5]):
                age = self.parse_age_from_filename(img_path.name)
                print(f"      {img_path.name} (å¹´é¾„: {age})")
            
            return True
        else:
            print(f"   âŒ çœŸå®æ•°æ®ä¸è¶³ï¼Œéœ€è¦è‡³å°‘100ä¸ªæœ‰æ•ˆæ–‡ä»¶")
            return False
    
    def is_valid_utkface_file(self, img_path: Path) -> bool:
        """éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆçš„UTKFaceæ–‡ä»¶"""
        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”ä¸ºå›¾åƒ
            if not img_path.exists() or img_path.suffix.lower() not in ['.jpg', '.jpeg']:
                return False
            
            # æ£€æŸ¥æ–‡ä»¶åæ ¼å¼
            filename = img_path.stem
            parts = filename.split('_')
            
            # UTKFaceæ ¼å¼: [age]_[gender]_[race]_[timestamp]
            if len(parts) >= 4:
                age = int(parts[0])
                gender = int(parts[1])
                race = int(parts[2])
                
                # éªŒè¯èŒƒå›´
                if 0 <= age <= 120 and 0 <= gender <= 1 and 0 <= race <= 4:
                    # å°è¯•åŠ è½½å›¾åƒéªŒè¯
                    try:
                        Image.open(img_path).convert('RGB')
                        return True
                    except:
                        return False
            
            return False
        except (ValueError, IndexError, Exception):
            return False
    
    def parse_age_from_filename(self, filename: str) -> Optional[int]:
        """ä»æ–‡ä»¶åè§£æå¹´é¾„"""
        try:
            parts = filename.split('_')
            if len(parts) >= 1:
                age = int(parts[0])
                return age if 0 <= age <= 120 else None
            return None
        except (ValueError, IndexError):
            return None

class RealUTKFaceCSVProcessor:
    """çœŸå®UTKFaceæ•°æ®CSVå¤„ç†å™¨"""
    
    def __init__(self, data_dir: str = "data"):
        self.data_dir = Path(data_dir)
        self.transform = transforms.Compose([
            transforms.Resize((128, 128)),
            transforms.ToTensor()
        ])
    
    def parse_utkface_info(self, filename: str) -> Optional[dict]:
        """è§£æUTKFaceæ–‡ä»¶ä¿¡æ¯"""
        try:
            name = os.path.splitext(filename)[0]
            parts = name.split('_')
            
            if len(parts) >= 4:
                return {
                    'age': int(parts[0]),
                    'gender': int(parts[1]),  # 0: å¥³æ€§, 1: ç”·æ€§
                    'race': int(parts[2]),    # 0: ç™½äºº, 1: é»‘äºº, 2: äºšæ´²äºº, 3: å°åº¦äºº, 4: å…¶ä»–
                    'timestamp': parts[3] if len(parts) > 3 else '0'
                }
            return None
        except (ValueError, IndexError):
            return None
    
    def extract_facial_features(self, image_path: Path) -> Optional[np.ndarray]:
        """ä»çœŸå®é¢éƒ¨å›¾åƒä¸­æå–30ç»´ç‰¹å¾"""
        try:
            # åŠ è½½å›¾åƒ
            image = Image.open(image_path).convert('RGB')
            tensor = self.transform(image)
            img_array = tensor.numpy()  # shape: (3, 128, 128)
            
            features = []
            
            # RGBé€šé“ç»Ÿè®¡ç‰¹å¾ (21ç»´)
            channel_names = ['R', 'G', 'B']
            for channel in range(3):
                channel_data = img_array[channel].flatten()
                
                # 7ä¸ªç»Ÿè®¡ç‰¹å¾
                channel_features = [
                    np.mean(channel_data),      # å‡å€¼
                    np.std(channel_data),       # æ ‡å‡†å·®
                    np.median(channel_data),    # ä¸­ä½æ•°
                    np.percentile(channel_data, 25),  # 25%åˆ†ä½æ•°
                    np.percentile(channel_data, 75),  # 75%åˆ†ä½æ•°
                    np.min(channel_data),       # æœ€å°å€¼
                    np.max(channel_data),       # æœ€å¤§å€¼
                ]
                features.extend(channel_features)
            
            # å…¨å±€ç»Ÿè®¡ç‰¹å¾ (5ç»´)
            all_pixels = img_array.flatten()
            global_features = [
                np.mean(all_pixels),                    # å…¨å±€å‡å€¼
                np.std(all_pixels),                     # å…¨å±€æ ‡å‡†å·®
                np.var(all_pixels),                     # å…¨å±€æ–¹å·®
                np.sum(all_pixels > np.mean(all_pixels)), # äº®åƒç´ æ•°
                np.sum(all_pixels < np.mean(all_pixels)), # æš—åƒç´ æ•°
            ]
            features.extend(global_features)
            
            # çº¹ç†ç‰¹å¾ (4ç»´) - åŸºäºæ¢¯åº¦
            gray = np.mean(img_array, axis=0)  # è½¬ä¸ºç°åº¦
            
            # è®¡ç®—å›¾åƒæ¢¯åº¦
            grad_x = np.diff(gray, axis=1)
            grad_y = np.diff(gray, axis=0)
            
            texture_features = [
                np.mean(np.abs(grad_x)),    # Xæ–¹å‘æ¢¯åº¦å‡å€¼
                np.mean(np.abs(grad_y)),    # Yæ–¹å‘æ¢¯åº¦å‡å€¼  
                np.std(grad_x),             # Xæ–¹å‘æ¢¯åº¦æ ‡å‡†å·®
                np.std(grad_y),             # Yæ–¹å‘æ¢¯åº¦æ ‡å‡†å·®
            ]
            features.extend(texture_features)
            
            return np.array(features)
            
        except Exception as e:
            print(f"   âŒ ç‰¹å¾æå–å¤±è´¥ {image_path.name}: {str(e)}")
            return None
    
    def load_real_utkface_dataset(self, max_samples: int = 2000) -> Tuple[np.ndarray, np.ndarray, List[str]]:
        """åŠ è½½çœŸå®UTKFaceæ•°æ®é›†"""
        print(f"ğŸ“‚ åŠ è½½çœŸå®UTKFaceæ•°æ®é›†...")
        
        # æœç´¢æ‰€æœ‰çœŸå®UTKFaceå›¾åƒ
        search_patterns = [
            self.data_dir / "*.jpg",
            self.data_dir / "*.jpeg",
            self.data_dir / "**/*.jpg", 
            self.data_dir / "**/*.jpeg",
            self.data_dir / "UTKFace" / "*.jpg",
            self.data_dir / "utkface-new" / "*.jpg",
            self.data_dir / "crop_part1" / "*.jpg",
        ]
        
        all_images = []
        for pattern in search_patterns:
            try:
                images = list(Path().glob(str(pattern)))
                all_images.extend(images)
            except:
                continue
        
        # å»é‡
        all_images = list(set(all_images))
        
        # è¿‡æ»¤å‡ºæœ‰æ•ˆçš„UTKFaceæ–‡ä»¶
        valid_images = []
        for img_path in all_images:
            info = self.parse_utkface_info(img_path.name)
            if info and 0 <= info['age'] <= 120:
                valid_images.append((img_path, info))
        
        if len(valid_images) == 0:
            raise ValueError("âŒ æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„çœŸå®UTKFaceå›¾åƒæ–‡ä»¶!")
        
        print(f"   âœ… æ‰¾åˆ° {len(valid_images)} ä¸ªæœ‰æ•ˆçš„çœŸå®UTKFaceå›¾åƒ")
        
        # é™åˆ¶æ ·æœ¬æ•°é‡å¹¶éšæœºæ‰“ä¹±
        if len(valid_images) > max_samples:
            import random
            random.shuffle(valid_images)
            valid_images = valid_images[:max_samples]
            print(f"   ğŸ“Š éšæœºé€‰æ‹© {max_samples} ä¸ªæ ·æœ¬")
        
        # æå–ç‰¹å¾å’Œæ ‡ç­¾
        features_list = []
        ages_list = []
        filenames_list = []
        
        print(f"   ğŸ”„ æ­£åœ¨ä»çœŸå®å›¾åƒæå–ç‰¹å¾...")
        processed = 0
        failed = 0
        
        for img_path, info in valid_images:
            features = self.extract_facial_features(img_path)
            
            if features is not None:
                features_list.append(features)
                ages_list.append(info['age'])
                filenames_list.append(img_path.name)
                processed += 1
            else:
                failed += 1
                
            if (processed + failed) % 100 == 0:
                print(f"   ğŸ“Š å·²å¤„ç†: {processed + failed}/{len(valid_images)} (æˆåŠŸ: {processed}, å¤±è´¥: {failed})")
        
        if len(features_list) == 0:
            raise ValueError("âŒ æ— æ³•ä»ä»»ä½•å›¾åƒä¸­æå–æœ‰æ•ˆç‰¹å¾!")
        
        print(f"   âœ… æˆåŠŸå¤„ç† {len(features_list)} ä¸ªçœŸå®æ ·æœ¬")
        print(f"   ğŸ“Š å¹´é¾„èŒƒå›´: {min(ages_list)}-{max(ages_list)} å²")
        print(f"   ğŸ“Š å¹³å‡å¹´é¾„: {np.mean(ages_list):.1f} å²")
        
        # æ˜¾ç¤ºå¹´é¾„åˆ†å¸ƒ
        ages_array = np.array(ages_list)
        print(f"   ğŸ“Š å¹´é¾„åˆ†å¸ƒ:")
        print(f"      0-20å²: {np.sum((ages_array >= 0) & (ages_array <= 20))} ä¸ª")
        print(f"      21-40å²: {np.sum((ages_array >= 21) & (ages_array <= 40))} ä¸ª")
        print(f"      41-60å²: {np.sum((ages_array >= 41) & (ages_array <= 60))} ä¸ª")
        print(f"      61+å²: {np.sum(ages_array > 60)} ä¸ª")
        
        return np.array(features_list), np.array(ages_list), filenames_list

def create_manual_real_utkface_csv() -> str:
    """åˆ›å»ºåŸºäºæ‰‹åŠ¨ä¸‹è½½çœŸå®UTKFaceæ•°æ®çš„CSVè¡¨æ ¼"""
    
    print("ğŸ¯ æ‰‹åŠ¨çœŸå®UTKFaceæ•°æ®CSVè¡¨æ ¼ç”Ÿæˆå™¨")
    print("=" * 70)
    print("ğŸ“‹ ç‰¹ç‚¹:")
    print("   âœ… 100%ä½¿ç”¨æ‰‹åŠ¨ä¸‹è½½çš„çœŸå®UTKFaceå›¾åƒ")
    print("   âœ… ç»å¯¹ä¸ä½¿ç”¨ä»»ä½•æ¨¡æ‹Ÿæˆ–ç”Ÿæˆæ•°æ®")
    print("   âœ… ç›´æ¥ä»çœŸå®äººè„¸å›¾åƒæå–30ç»´ç‰¹å¾")
    print("   âœ… ä½¿ç”¨å›¾åƒæ–‡ä»¶åä¸­çš„çœŸå®å¹´é¾„æ ‡ç­¾")
    print("=" * 70)
    
    # 1. æ£€æŸ¥çœŸå®æ•°æ®
    checker = ManualUTKFaceDataChecker("data")
    
    if not checker.check_real_data():
        print("\nâŒ æœªæ‰¾åˆ°è¶³å¤Ÿçš„çœŸå®UTKFaceæ•°æ®!")
        checker.show_download_instructions()
        print("\nğŸ’¡ è¯·æŒ‰ç…§ä¸Šè¿°æŒ‡å—ä¸‹è½½çœŸå®æ•°æ®åé‡æ–°è¿è¡Œæ­¤è„šæœ¬")
        raise ValueError("éœ€è¦æ‰‹åŠ¨ä¸‹è½½çœŸå®UTKFaceæ•°æ®é›†")
    
    # 2. åŠ è½½çœŸå®æ•°æ®é›†
    processor = RealUTKFaceCSVProcessor("data")
    features, ages, filenames = processor.load_real_utkface_dataset(max_samples=2000)
    
    print(f"\nğŸ“Š çœŸå®æ•°æ®é›†ç»Ÿè®¡:")
    print(f"   æ ·æœ¬æ•°é‡: {len(features)} (100%çœŸå®)")
    print(f"   ç‰¹å¾ç»´åº¦: {features.shape[1]}")
    print(f"   æ•°æ®æ¥æº: æ‰‹åŠ¨ä¸‹è½½çš„çœŸå®UTKFaceå›¾åƒ")
    
    # 3. ç‰¹å¾æ ‡å‡†åŒ–
    print(f"\nğŸ”§ æ•°æ®é¢„å¤„ç†...")
    scaler = StandardScaler()
    features_scaled = scaler.fit_transform(features)
    
    # 4. æ•°æ®åˆ’åˆ†
    test_size = min(0.25, 300/len(features))  # æœ€å¤š25%æˆ–300ä¸ªæ ·æœ¬ä½œä¸ºæµ‹è¯•é›†
    X_train, X_test, y_train, y_test, files_train, files_test = train_test_split(
        features_scaled, ages, filenames, test_size=test_size, random_state=42
    )
    
    print(f"   è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬ (çœŸå®)")
    print(f"   æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬ (çœŸå®)")
    
    # 5. è®­ç»ƒå¹´é¾„é¢„æµ‹æ¨¡å‹
    print(f"\nğŸ¯ è®­ç»ƒå¹´é¾„é¢„æµ‹æ¨¡å‹...")
    model = RandomForestRegressor(
        n_estimators=300,
        max_depth=20,
        random_state=42,
        min_samples_split=3,
        min_samples_leaf=1,
        max_features='sqrt',
        n_jobs=-1
    )
    model.fit(X_train, y_train)
    
    # 6. é¢„æµ‹å’Œè¯„ä¼°
    test_pred = model.predict(X_test)
    train_pred = model.predict(X_train)
    
    test_mae = mean_absolute_error(y_test, test_pred)
    test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))
    train_mae = mean_absolute_error(y_train, train_pred)
    correlation = np.corrcoef(y_test, test_pred)[0,1]
    
    print(f"\nğŸ“ˆ æ¨¡å‹æ€§èƒ½ (åŸºäºçœŸå®UTKFaceæ•°æ®):")
    print(f"   æµ‹è¯•é›† MAE: {test_mae:.3f} å²")
    print(f"   æµ‹è¯•é›† RMSE: {test_rmse:.3f} å²")
    print(f"   è®­ç»ƒé›† MAE: {train_mae:.3f} å²")
    print(f"   ç›¸å…³ç³»æ•°: {correlation:.3f}")
    
    # 7. åˆ›å»ºCSVè¡¨æ ¼
    print(f"\nğŸ“‹ åˆ›å»ºCSVè¡¨æ ¼...")
    
    # ç”Ÿæˆç‰¹å¾åˆ—åï¼ˆ30ç»´ï¼‰
    feature_names = []
    # RGBé€šé“ç»Ÿè®¡ç‰¹å¾ (21ç»´)
    for channel in ['R', 'G', 'B']:
        for stat in ['mean', 'std', 'median', 'q25', 'q75', 'min', 'max']:
            feature_names.append(f'{channel}_{stat}')
    
    # å…¨å±€ç»Ÿè®¡ç‰¹å¾ (5ç»´)
    for stat in ['global_mean', 'global_std', 'global_var', 'bright_pixels', 'dark_pixels']:
        feature_names.append(stat)
    
    # çº¹ç†ç‰¹å¾ (4ç»´)
    for stat in ['grad_x_mean', 'grad_y_mean', 'grad_x_std', 'grad_y_std']:
        feature_names.append(stat)
    
    # æ„å»ºè¡¨æ ¼æ•°æ®
    table_data = {}
    
    # æ·»åŠ 30ç»´ç‰¹å¾
    for i, feature_name in enumerate(feature_names):
        table_data[feature_name] = X_test[:, i]
    
    # æ·»åŠ é¢„æµ‹ç»“æœ
    abs_errors = np.abs(test_pred - y_test)
    table_data['Predicted_Age'] = np.round(test_pred, 2)
    table_data['Actual_Age'] = y_test
    table_data['Abs_Error'] = np.round(abs_errors, 2)
    table_data['Filename'] = files_test
    
    # åˆ›å»ºDataFrameå¹¶æŒ‰è¯¯å·®æ’åº
    df = pd.DataFrame(table_data)
    df = df.sort_values('Abs_Error').reset_index(drop=True)
    
    print(f"âœ… CSVè¡¨æ ¼åˆ›å»ºå®Œæˆ")
    print(f"   è¡Œæ•°: {len(df)} (å…¨éƒ¨ä¸ºçœŸå®æµ‹è¯•æ ·æœ¬)")
    print(f"   åˆ—æ•°: {len(df.columns)}")
    print(f"   æ•°æ®æ¥æº: 100%æ‰‹åŠ¨ä¸‹è½½çš„çœŸå®UTKFaceå›¾åƒ")
    
    # 8. ä¿å­˜ç»“æœ
    output_dir = Path('results/metrics')
    output_dir.mkdir(parents=True, exist_ok=True)
    output_path = output_dir / 'manual_real_utkface_features.csv'
    
    # é‡æ–°æ’åˆ—åˆ—çš„é¡ºåºï¼š30ç»´ç‰¹å¾åœ¨å‰ï¼Œç»“æœåœ¨å
    feature_cols = feature_names
    result_cols = ['Predicted_Age', 'Actual_Age', 'Abs_Error']
    final_cols = feature_cols + result_cols
    
    final_df = df[final_cols].copy()
    final_df.to_csv(output_path, index=False, encoding='utf-8')
    
    print(f"\nğŸ’¾ çœŸå®æ•°æ®CSVå·²ä¿å­˜: {output_path}")
    
    # æ˜¾ç¤ºç»“æœé¢„è§ˆ
    print(f"\nğŸ“‹ çœŸå®æ•°æ®è¡¨æ ¼é¢„è§ˆ (å‰5è¡Œ, å‰8åˆ—):")
    preview_cols = feature_cols[:5] + result_cols
    print(final_df[preview_cols].head().to_string(index=False, float_format='%.3f'))
    
    # æ€§èƒ½ç»Ÿè®¡
    print(f"\nğŸ“Š çœŸå®æ•°æ®æ€§èƒ½ç»Ÿè®¡:")
    abs_errors = final_df['Abs_Error']
    print(f"   æ ·æœ¬æ€»æ•°: {len(final_df)} (100%çœŸå®UTKFace)")
    print(f"   å¹³å‡ç»å¯¹è¯¯å·®: {abs_errors.mean():.3f} å²")
    print(f"   ä¸­ä½æ•°è¯¯å·®: {abs_errors.median():.3f} å²")
    print(f"   æœ€å¤§è¯¯å·®: {abs_errors.max():.3f} å²")
    print(f"   æœ€å°è¯¯å·®: {abs_errors.min():.3f} å²")
    print(f"   æ ‡å‡†å·®: {abs_errors.std():.3f} å²")
    
    # è¯¯å·®åˆ†å¸ƒåˆ†æ
    excellent = np.sum(abs_errors <= 3)
    good = np.sum((abs_errors > 3) & (abs_errors <= 6))
    fair = np.sum((abs_errors > 6) & (abs_errors <= 10))
    poor = np.sum(abs_errors > 10)
    
    print(f"\nğŸ¯ çœŸå®æ•°æ®è¯¯å·®åˆ†å¸ƒ:")
    print(f"   ä¼˜ç§€ (â‰¤3å²): {excellent} ä¸ª ({excellent/len(abs_errors)*100:.1f}%)")
    print(f"   è‰¯å¥½ (3-6å²): {good} ä¸ª ({good/len(abs_errors)*100:.1f}%)")
    print(f"   ä¸€èˆ¬ (6-10å²): {fair} ä¸ª ({fair/len(abs_errors)*100:.1f}%)")
    print(f"   è¾ƒå·® (>10å²): {poor} ä¸ª ({poor/len(abs_errors)*100:.1f}%)")
    
    print(f"\nğŸ‰ æ‰‹åŠ¨çœŸå®UTKFaceæ•°æ®CSVè¡¨æ ¼ç”Ÿæˆå®Œæˆï¼")
    print(f"ğŸ“ æ–‡ä»¶è·¯å¾„: {output_path}")
    print(f"ğŸ“‹ æ ¼å¼: 30ç»´çœŸå®ç‰¹å¾ | é¢„æµ‹å¹´é¾„ | çœŸå®å¹´é¾„ | ç»å¯¹è¯¯å·®")
    print(f"âœ¨ æ•°æ®ä¿è¯: 100%åŸºäºæ‰‹åŠ¨ä¸‹è½½çš„çœŸå®UTKFaceå›¾åƒ")
    print(f"ğŸš« ä¸å«ä»»ä½•: æ¨¡æ‹Ÿæ•°æ®ã€ç”Ÿæˆæ•°æ®æˆ–äººå·¥åˆæˆæ•°æ®")
    
    return str(output_path)

def main():
    """ä¸»å‡½æ•°"""
    try:
        result_path = create_manual_real_utkface_csv()
        print(f"\nâœ… æˆåŠŸ! çœŸå®æ•°æ®CSVæ–‡ä»¶å·²ç”Ÿæˆ: {result_path}")
        
    except Exception as e:
        print(f"\nâŒ å¤„ç†å¤±è´¥: {str(e)}")
        
        if "éœ€è¦æ‰‹åŠ¨ä¸‹è½½" in str(e):
            print(f"\nğŸ“ è§£å†³æ­¥éª¤:")
            print(f"   1. æŒ‰ç…§ä¸Šé¢çš„æŒ‡å—ä¸‹è½½çœŸå®UTKFaceæ•°æ®é›†")
            print(f"   2. å°†æ‰€æœ‰.jpgæ–‡ä»¶æ”¾å…¥ data/ ç›®å½•")
            print(f"   3. é‡æ–°è¿è¡Œæ­¤è„šæœ¬")
            print(f"\nğŸ”— ä¸‹è½½é“¾æ¥:")
            print(f"   - Kaggle: https://www.kaggle.com/datasets/jangedoo/utkface-new")
            print(f"   - å®˜æ–¹: https://susanqq.github.io/UTKFace/")
        else:
            import traceback
            traceback.print_exc()
        
        return None

if __name__ == "__main__":
    main() 