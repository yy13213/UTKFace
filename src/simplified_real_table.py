import numpy as np
import pandas as pd
import torch
import torchvision.transforms as transforms
from PIL import Image
import os
import glob
from typing import Tuple, List, Optional
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error
import warnings
warnings.filterwarnings('ignore')

class SimplifiedUTKFaceProcessor:
    """ç®€åŒ–çš„UTKFaceæ•°æ®å¤„ç†å™¨"""
    
    def __init__(self, data_path: str = "data"):
        self.data_path = data_path
        
    def parse_filename(self, filename: str) -> Optional[int]:
        """è§£æUTKFaceæ–‡ä»¶åä¸­çš„å¹´é¾„"""
        try:
            basename = os.path.splitext(filename)[0]
            parts = basename.split('_')
            if len(parts) >= 1:
                age = int(parts[0])
                return age if 0 <= age <= 120 else None
            return None
        except (ValueError, IndexError):
            return None
    
    def extract_basic_features(self, image_path: str) -> Optional[np.ndarray]:
        """æå–åŸºæœ¬å›¾åƒç‰¹å¾"""
        try:
            # åŠ è½½å›¾åƒ
            image = Image.open(image_path).convert('RGB')
            image = image.resize((64, 64))  # ç®€åŒ–ä¸º64x64
            
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            img_array = np.array(image) / 255.0
            
            # æå–ç»Ÿè®¡ç‰¹å¾
            features = []
            
            # RGBé€šé“çš„ç»Ÿè®¡ç‰¹å¾
            for channel in range(3):
                channel_data = img_array[:, :, channel]
                features.extend([
                    np.mean(channel_data),
                    np.std(channel_data),
                    np.median(channel_data),
                    np.percentile(channel_data, 25),
                    np.percentile(channel_data, 75)
                ])
            
            # ç°åº¦ç»Ÿè®¡ç‰¹å¾
            gray = np.mean(img_array, axis=2)
            features.extend([
                np.mean(gray),
                np.std(gray),
                np.var(gray)
            ])
            
            return np.array(features)
            
        except Exception as e:
            print(f"   é”™è¯¯å¤„ç† {image_path}: {str(e)}")
            return None
    
    def load_data(self, max_samples: int = 100) -> Tuple[np.ndarray, np.ndarray, List[str]]:
        """åŠ è½½æ•°æ®"""
        print(f"ğŸ” åœ¨ {self.data_path} ä¸­æœç´¢UTKFaceå›¾åƒæ–‡ä»¶...")
        
        # æŸ¥æ‰¾å›¾åƒæ–‡ä»¶
        image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp']
        image_files = []
        
        for ext in image_extensions:
            files = glob.glob(os.path.join(self.data_path, ext))
            image_files.extend(files)
        
        if len(image_files) == 0:
            print("âŒ æœªæ‰¾åˆ°å›¾åƒæ–‡ä»¶ï¼Œç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®...")
            return self._generate_mock_data(max_samples)
        
        print(f"ğŸ“¸ æ‰¾åˆ° {len(image_files)} ä¸ªå›¾åƒæ–‡ä»¶")
        
        # å¤„ç†å›¾åƒæ–‡ä»¶
        features_list = []
        ages_list = []
        filenames_list = []
        
        processed_count = 0
        
        for img_path in image_files:
            if processed_count >= max_samples:
                break
                
            filename = os.path.basename(img_path)
            age = self.parse_filename(filename)
            
            if age is None:
                continue
                
            features = self.extract_basic_features(img_path)
            if features is None:
                continue
            
            features_list.append(features)
            ages_list.append(age)
            filenames_list.append(filename)
            processed_count += 1
            
            if processed_count % 20 == 0:
                print(f"   å·²å¤„ç† {processed_count} ä¸ªæ ·æœ¬")
        
        if len(features_list) == 0:
            print("âŒ æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•çœŸå®æ•°æ®ï¼Œç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®...")
            return self._generate_mock_data(max_samples)
        
        print(f"âœ… æˆåŠŸå¤„ç† {len(features_list)} ä¸ªçœŸå®æ ·æœ¬")
        
        return np.array(features_list), np.array(ages_list), filenames_list
    
    def _generate_mock_data(self, max_samples: int) -> Tuple[np.ndarray, np.ndarray, List[str]]:
        """ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®"""
        np.random.seed(42)
        
        # ç”Ÿæˆ18ç»´ç‰¹å¾
        features = np.random.randn(max_samples, 18)
        
        # ç”Ÿæˆå¹´é¾„ï¼ˆåŸºäºç‰¹å¾çš„åˆç†ç»„åˆï¼‰
        age_base = np.dot(features[:, :5], [2, -1, 1.5, 0.5, -0.8]) + 40
        ages = np.clip(age_base + np.random.normal(0, 8, max_samples), 1, 99).astype(int)
        
        # ç”Ÿæˆæ–‡ä»¶å
        filenames = [f"{age}_{np.random.randint(0,2)}_{np.random.randint(0,5)}_demo_{i:03d}.jpg" 
                    for i, age in enumerate(ages)]
        
        print(f"âœ… ç”Ÿæˆäº† {max_samples} ä¸ªæ¨¡æ‹Ÿæ ·æœ¬")
        return features, ages, filenames

def create_prediction_results_csv(data_path: str = "data", 
                                max_samples: int = 100,
                                test_ratio: float = 0.3):
    """åˆ›å»ºé¢„æµ‹ç»“æœCSVæ–‡ä»¶"""
    
    print("ğŸ¯ å¼€å§‹åˆ›å»ºUTKFaceé¢„æµ‹ç»“æœè¡¨æ ¼")
    print("=" * 50)
    
    # 1. åŠ è½½æ•°æ®
    processor = SimplifiedUTKFaceProcessor(data_path)
    features, ages, filenames = processor.load_data(max_samples)
    
    print(f"\nğŸ“Š æ•°æ®æ¦‚è§ˆ:")
    print(f"   æ ·æœ¬æ•°é‡: {len(features)}")
    print(f"   ç‰¹å¾ç»´åº¦: {features.shape[1]}")
    print(f"   å¹´é¾„èŒƒå›´: {ages.min()} - {ages.max()} å²")
    print(f"   å¹³å‡å¹´é¾„: {ages.mean():.1f} å²")
    
    # 2. æ•°æ®é¢„å¤„ç†
    if len(features) >= 10:
        # åˆ†å‰²è®­ç»ƒæµ‹è¯•é›†
        X_train, X_test, y_train, y_test, files_train, files_test = train_test_split(
            features, ages, filenames, test_size=test_ratio, random_state=42
        )
        print(f"\nğŸ“Š æ•°æ®åˆ†å‰²:")
        print(f"   è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
        print(f"   æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")
    else:
        # æ ·æœ¬å¤ªå°‘ï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®
        X_train = X_test = features
        y_train = y_test = ages
        files_train = files_test = filenames
        print(f"\nâš ï¸  æ ·æœ¬è¾ƒå°‘ï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®è¿›è¡Œæ¼”ç¤º")
    
    # 3. ç‰¹å¾é™ç»´
    n_components = min(8, X_train.shape[1] - 1, len(X_train) - 1)
    pca = PCA(n_components=n_components)
    X_train_pca = pca.fit_transform(X_train)
    X_test_pca = pca.transform(X_test)
    
    print(f"\nğŸ”„ PCAé™ç»´: {X_train.shape[1]} -> {n_components}")
    print(f"   ç´¯è®¡æ–¹å·®è§£é‡Šæ¯”: {pca.explained_variance_ratio_.sum():.3f}")
    
    # 4. è®­ç»ƒé¢„æµ‹æ¨¡å‹
    model = RandomForestRegressor(n_estimators=50, random_state=42, max_depth=5)
    model.fit(X_train_pca, y_train)
    
    # è®­ç»ƒè¯¯å·®
    train_pred = model.predict(X_train_pca)
    train_mae = mean_absolute_error(y_train, train_pred)
    print(f"\nğŸ“ˆ è®­ç»ƒæ€§èƒ½: MAE = {train_mae:.2f} å²")
    
    # 5. æµ‹è¯•é›†é¢„æµ‹
    test_pred = model.predict(X_test_pca)
    test_mae = mean_absolute_error(y_test, test_pred)
    test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))
    
    print(f"ğŸ“ˆ æµ‹è¯•æ€§èƒ½:")
    print(f"   MAE: {test_mae:.2f} å²")
    print(f"   RMSE: {test_rmse:.2f} å²")
    
    # 6. è®¡ç®—è¯¯å·®æŒ‡æ ‡
    abs_errors = np.abs(test_pred - y_test)
    # ä¿®æ­£ç›¸å¯¹è¯¯å·®è®¡ç®—ï¼Œé¿å…é™¤ä»¥æ¥è¿‘0çš„æ•°
    rel_errors = np.where(y_test > 0, (abs_errors / y_test) * 100, abs_errors * 100)
    rel_errors = np.clip(rel_errors, 0, 1000)  # é™åˆ¶æœ€å¤§ç›¸å¯¹è¯¯å·®ä¸º1000%
    
    # 7. åˆ›å»ºç»“æœè¡¨æ ¼
    results_data = {
        'æ ·æœ¬ç¼–å·': range(1, len(y_test) + 1),
        'æ–‡ä»¶å': files_test,
    }
    
    # æ·»åŠ PCAç‰¹å¾
    for i in range(n_components):
        results_data[f'PC{i+1}'] = X_test_pca[:, i]
    
    # æ·»åŠ é¢„æµ‹ç»“æœ
    results_data.update({
        'é¢„æµ‹å¹´é¾„': np.round(test_pred, 1),
        'çœŸå®å¹´é¾„': y_test,
        'ç»å¯¹è¯¯å·®': np.round(abs_errors, 2),
        'ç›¸å¯¹è¯¯å·®(%)': np.round(rel_errors, 1),
        'è¯¯å·®ç­‰çº§': ['ä½' if e <= 3 else 'ä¸­' if e <= 8 else 'é«˜' for e in abs_errors]
    })
    
    # åˆ›å»ºDataFrame
    df = pd.DataFrame(results_data)
    
    # æŒ‰ç»å¯¹è¯¯å·®æ’åº
    df = df.sort_values('ç»å¯¹è¯¯å·®').reset_index(drop=True)
    df['æ ·æœ¬ç¼–å·'] = range(1, len(df) + 1)
    
    return df

def save_comprehensive_results(df: pd.DataFrame, 
                             base_path: str = 'results/metrics/'):
    """ä¿å­˜å®Œæ•´çš„ç»“æœæ–‡ä»¶"""
    
    os.makedirs(base_path, exist_ok=True)
    
    # 1. è¯¦ç»†ç»“æœCSV
    detail_path = os.path.join(base_path, 'utkface_detailed_results.csv')
    df.to_csv(detail_path, index=False, encoding='utf-8-sig')
    
    # 2. åˆ›å»ºæ±‡æ€»ç»Ÿè®¡
    summary_stats = {
        'æŒ‡æ ‡': [
            'æ€»æ ·æœ¬æ•°', 'å¹³å‡ç»å¯¹è¯¯å·®(å²)', 'è¯¯å·®æ ‡å‡†å·®(å²)', 
            'ä¸­ä½æ•°è¯¯å·®(å²)', 'æœ€å°è¯¯å·®(å²)', 'æœ€å¤§è¯¯å·®(å²)',
            'å¹³å‡ç›¸å¯¹è¯¯å·®(%)', 'RMSE(å²)', 'é¢„æµ‹å¹³å‡å€¼', 'çœŸå®å¹³å‡å€¼'
        ],
        'æ•°å€¼': [
            len(df),
            f"{df['ç»å¯¹è¯¯å·®'].mean():.2f}",
            f"{df['ç»å¯¹è¯¯å·®'].std():.2f}",
            f"{df['ç»å¯¹è¯¯å·®'].median():.2f}",
            f"{df['ç»å¯¹è¯¯å·®'].min():.2f}",
            f"{df['ç»å¯¹è¯¯å·®'].max():.2f}",
            f"{df['ç›¸å¯¹è¯¯å·®(%)'].mean():.1f}%",
            f"{np.sqrt(np.mean((df['é¢„æµ‹å¹´é¾„'] - df['çœŸå®å¹´é¾„'])**2)):.2f}",
            f"{df['é¢„æµ‹å¹´é¾„'].mean():.1f}",
            f"{df['çœŸå®å¹´é¾„'].mean():.1f}"
        ]
    }
    
    summary_df = pd.DataFrame(summary_stats)
    summary_path = os.path.join(base_path, 'utkface_summary_stats.csv')
    summary_df.to_csv(summary_path, index=False, encoding='utf-8-sig')
    
    # 3. è¯¯å·®åˆ†å¸ƒç»Ÿè®¡
    error_bins = [0, 2, 5, 10, 20, float('inf')]
    error_labels = ['ä¼˜ç§€(0-2å²)', 'è‰¯å¥½(2-5å²)', 'ä¸€èˆ¬(5-10å²)', 'è¾ƒå·®(10-20å²)', 'å¾ˆå·®(>20å²)']
    
    distribution_data = []
    for i, label in enumerate(error_labels):
        if i < len(error_bins) - 1:
            mask = (df['ç»å¯¹è¯¯å·®'] >= error_bins[i]) & (df['ç»å¯¹è¯¯å·®'] < error_bins[i+1])
        else:
            mask = df['ç»å¯¹è¯¯å·®'] >= error_bins[i]
        
        count = mask.sum()
        percentage = count / len(df) * 100
        
        distribution_data.append({
            'è¯¯å·®èŒƒå›´': label,
            'æ ·æœ¬æ•°': count,
            'å æ¯”(%)': f"{percentage:.1f}%"
        })
    
    dist_df = pd.DataFrame(distribution_data)
    dist_path = os.path.join(base_path, 'utkface_error_distribution.csv')
    dist_df.to_csv(dist_path, index=False, encoding='utf-8-sig')
    
    # 4. è¾“å‡ºç»“æœä¿¡æ¯
    print(f"\nğŸ“ ç»“æœæ–‡ä»¶å·²ä¿å­˜:")
    print(f"   ğŸ“„ è¯¦ç»†ç»“æœ: {detail_path}")
    print(f"   ğŸ“Š ç»Ÿè®¡æ±‡æ€»: {summary_path}")
    print(f"   ğŸ“ˆ è¯¯å·®åˆ†å¸ƒ: {dist_path}")
    
    print(f"\nğŸ“‹ ç»“æœé¢„è§ˆ (å‰10è¡Œ):")
    display_cols = ['æ ·æœ¬ç¼–å·', 'æ–‡ä»¶å', 'é¢„æµ‹å¹´é¾„', 'çœŸå®å¹´é¾„', 'ç»å¯¹è¯¯å·®', 'ç›¸å¯¹è¯¯å·®(%)', 'è¯¯å·®ç­‰çº§']
    print(df[display_cols].head(10).to_string(index=False))
    
    print(f"\nğŸ“Š æ€§èƒ½æ¦‚è§ˆ:")
    print(f"   ğŸ¯ å¹³å‡ç»å¯¹è¯¯å·®: {df['ç»å¯¹è¯¯å·®'].mean():.2f} å²")
    print(f"   ğŸ“Š è¯¯å·®æ ‡å‡†å·®: {df['ç»å¯¹è¯¯å·®'].std():.2f} å²")
    print(f"   ğŸ”„ ç›¸å¯¹è¯¯å·®: {df['ç›¸å¯¹è¯¯å·®(%)'].mean():.1f}%")
    print(f"   ğŸ“ˆ ä¼˜ç§€æ ·æœ¬ (â‰¤2å²è¯¯å·®): {(df['ç»å¯¹è¯¯å·®'] <= 2).sum()}/{len(df)} ({(df['ç»å¯¹è¯¯å·®'] <= 2).mean()*100:.1f}%)")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ UTKFaceç®€åŒ–ç‰ˆç»“æœè¡¨æ ¼ç”Ÿæˆ")
    
    # æ£€æŸ¥æ•°æ®ç›®å½•
    data_path = "data"
    if not os.path.exists(data_path):
        os.makedirs(data_path, exist_ok=True)
        print(f"âš ï¸  åˆ›å»ºæ•°æ®ç›®å½•: {data_path}")
    
    try:
        # ç”Ÿæˆç»“æœè¡¨æ ¼
        results_df = create_prediction_results_csv(
            data_path=data_path,
            max_samples=150,  # æœ€å¤šå¤„ç†150ä¸ªæ ·æœ¬
            test_ratio=0.3    # 30%ç”¨äºæµ‹è¯•
        )
        
        # ä¿å­˜ç»“æœæ–‡ä»¶
        save_comprehensive_results(results_df)
        
        print(f"\nğŸ‰ å¤„ç†å®Œæˆï¼æ‰€æœ‰CSVæ–‡ä»¶å·²ç”Ÿæˆã€‚")
        
    except Exception as e:
        print(f"âŒ å‡ºç°é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 