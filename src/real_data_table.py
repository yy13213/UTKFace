import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torchvision.transforms as transforms
from PIL import Image
import os
import glob
from typing import Tuple, List, Optional
import re
from sklearn.decomposition import PCA
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error
import warnings
warnings.filterwarnings('ignore')

class UTKFaceRealDataProcessor:
    """UTKFaceçœŸå®žæ•°æ®å¤„ç†å™¨"""
    
    def __init__(self, data_path: str = "data"):
        self.data_path = data_path
        self.transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
        
    def parse_filename(self, filename: str) -> Tuple[Optional[int], Optional[int], Optional[int]]:
        """
        è§£æžUTKFaceæ–‡ä»¶å
        æ ¼å¼: [age]_[gender]_[race]_[date&time].jpg
        
        Returns:
            age, gender, race (å¦‚æžœè§£æžå¤±è´¥è¿”å›žNone)
        """
        try:
            # ç§»é™¤æ–‡ä»¶æ‰©å±•å
            basename = os.path.splitext(filename)[0]
            parts = basename.split('_')
            
            if len(parts) >= 3:
                age = int(parts[0])
                gender = int(parts[1])
                race = int(parts[2])
                return age, gender, race
            else:
                return None, None, None
        except (ValueError, IndexError):
            return None, None, None
    
    def load_sample_data(self, max_samples: int = 200) -> Tuple[np.ndarray, np.ndarray, List[str], List[int]]:
        """
        åŠ è½½æ ·æœ¬æ•°æ®ï¼ˆå¦‚æžœæ²¡æœ‰çœŸå®žæ•°æ®ï¼Œç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ï¼‰
        
        Args:
            max_samples: æœ€å¤§æ ·æœ¬æ•°
            
        Returns:
            features: ç‰¹å¾æ•°ç»„ (n_samples, n_features)
            ages: å¹´é¾„æ•°ç»„ (n_samples,)
            filenames: æ–‡ä»¶ååˆ—è¡¨
            sample_ids: æ ·æœ¬IDåˆ—è¡¨
        """
        # å°è¯•åŠ è½½çœŸå®žæ•°æ®
        if os.path.exists(self.data_path):
            image_files = glob.glob(os.path.join(self.data_path, "*.jpg"))
            if len(image_files) > 0:
                print(f"ðŸŽ¯ å‘çŽ° {len(image_files)} ä¸ªçœŸå®žå›¾åƒæ–‡ä»¶")
                return self._load_real_data(image_files, max_samples)
        
        # å¦‚æžœæ²¡æœ‰çœŸå®žæ•°æ®ï¼Œç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
        print("âš ï¸  æœªæ‰¾åˆ°çœŸå®žUTKFaceæ•°æ®ï¼Œç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®è¿›è¡Œæ¼”ç¤º...")
        return self._generate_simulation_data(max_samples)
    
    def _load_real_data(self, image_files: List[str], max_samples: int) -> Tuple[np.ndarray, np.ndarray, List[str], List[int]]:
        """åŠ è½½çœŸå®žå›¾åƒæ•°æ®"""
        features_list = []
        ages_list = []
        filenames_list = []
        sample_ids = []
        
        # é™åˆ¶å¤„ç†çš„æ–‡ä»¶æ•°é‡
        if len(image_files) > max_samples:
            image_files = image_files[:max_samples]
        
        print(f"ðŸ“¸ æ­£åœ¨å¤„ç† {len(image_files)} ä¸ªå›¾åƒæ–‡ä»¶...")
        
        for i, img_path in enumerate(image_files):
            try:
                # è§£æžæ–‡ä»¶åèŽ·å–å¹´é¾„
                filename = os.path.basename(img_path)
                age, gender, race = self.parse_filename(filename)
                
                if age is None:
                    continue
                
                # åŠ è½½å’Œé¢„å¤„ç†å›¾åƒ
                image = Image.open(img_path).convert('RGB')
                tensor = self.transform(image)
                
                # æå–ç®€å•çš„å›¾åƒç‰¹å¾ï¼ˆå‡å€¼ã€æ–¹å·®ç­‰ï¼‰
                features = self._extract_simple_features(tensor)
                
                features_list.append(features)
                ages_list.append(age)
                filenames_list.append(filename)
                sample_ids.append(i)
                
                if len(features_list) % 50 == 0:
                    print(f"   å·²å¤„ç† {len(features_list)} ä¸ªæ ·æœ¬...")
                    
            except Exception as e:
                print(f"   è·³è¿‡æ–‡ä»¶ {filename}: {str(e)}")
                continue
        
        if len(features_list) == 0:
            print("âŒ æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•çœŸå®žæ•°æ®ï¼Œè½¬ä¸ºç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®")
            return self._generate_simulation_data(max_samples)
        
        features = np.array(features_list)
        ages = np.array(ages_list)
        
        print(f"âœ… æˆåŠŸåŠ è½½ {len(features)} ä¸ªçœŸå®žæ ·æœ¬")
        
        return features, ages, filenames_list, sample_ids
    
    def _extract_simple_features(self, tensor: torch.Tensor) -> np.ndarray:
        """æå–ç®€å•çš„å›¾åƒç‰¹å¾"""
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        img_array = tensor.numpy()
        
        features = []
        
        # å¯¹æ¯ä¸ªé€šé“è®¡ç®—ç»Ÿè®¡ç‰¹å¾
        for channel in range(img_array.shape[0]):
            channel_data = img_array[channel]
            
            # åŸºæœ¬ç»Ÿè®¡ç‰¹å¾
            features.extend([
                np.mean(channel_data),      # å‡å€¼
                np.std(channel_data),       # æ ‡å‡†å·®
                np.min(channel_data),       # æœ€å°å€¼
                np.max(channel_data),       # æœ€å¤§å€¼
                np.median(channel_data),    # ä¸­ä½æ•°
            ])
        
        # å…¨å±€ç‰¹å¾
        all_pixels = img_array.flatten()
        features.extend([
            np.mean(all_pixels),
            np.std(all_pixels),
            np.percentile(all_pixels, 25),  # 25%åˆ†ä½æ•°
            np.percentile(all_pixels, 75),  # 75%åˆ†ä½æ•°
            np.sum(all_pixels > 0),         # éžé›¶åƒç´ æ•°
        ])
        
        return np.array(features)
    
    def _generate_simulation_data(self, max_samples: int) -> Tuple[np.ndarray, np.ndarray, List[str], List[int]]:
        """ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®"""
        np.random.seed(42)
        
        # ç”Ÿæˆæ¨¡æ‹Ÿç‰¹å¾ï¼ˆ20ç»´ï¼‰
        features = np.random.randn(max_samples, 20)
        
        # ç”Ÿæˆæ¨¡æ‹Ÿå¹´é¾„ï¼ˆåŸºäºŽç‰¹å¾çš„çº¿æ€§ç»„åˆåŠ å™ªå£°ï¼‰
        age_coeffs = np.random.randn(20) * 0.5
        base_ages = np.dot(features, age_coeffs) * 10 + 40  # ä¸­å¿ƒå¹´é¾„40å²
        ages = np.clip(base_ages + np.random.normal(0, 5, max_samples), 0, 100).astype(int)
        
        # ç”Ÿæˆæ¨¡æ‹Ÿæ–‡ä»¶å
        filenames = []
        for i in range(max_samples):
            gender = np.random.randint(0, 2)
            race = np.random.randint(0, 5)
            timestamp = f"20200101_{i:04d}"
            filename = f"{ages[i]}_{gender}_{race}_{timestamp}.jpg"
            filenames.append(filename)
        
        sample_ids = list(range(max_samples))
        
        print(f"âœ… ç”Ÿæˆäº† {max_samples} ä¸ªæ¨¡æ‹Ÿæ ·æœ¬")
        
        return features, ages, filenames, sample_ids

class AgePredictor:
    """å¹´é¾„é¢„æµ‹å™¨"""
    
    def __init__(self, use_pca: bool = True, n_components: int = 10):
        self.use_pca = use_pca
        self.n_components = n_components
        self.pca = PCA(n_components=n_components) if use_pca else None
        self.regressor = RandomForestRegressor(n_estimators=100, random_state=42)
        self.is_fitted = False
    
    def fit(self, features: np.ndarray, ages: np.ndarray):
        """è®­ç»ƒé¢„æµ‹æ¨¡åž‹"""
        print("ðŸŽ¯ è®­ç»ƒå¹´é¾„é¢„æµ‹æ¨¡åž‹...")
        
        # PCAé™ç»´
        if self.use_pca:
            features_transformed = self.pca.fit_transform(features)
            print(f"   PCAé™ç»´: {features.shape[1]} -> {self.n_components}")
        else:
            features_transformed = features
        
        # è®­ç»ƒå›žå½’æ¨¡åž‹
        self.regressor.fit(features_transformed, ages)
        self.is_fitted = True
        
        # è®¡ç®—è®­ç»ƒè¯¯å·®
        train_pred = self.regressor.predict(features_transformed)
        train_mae = mean_absolute_error(ages, train_pred)
        train_rmse = np.sqrt(mean_squared_error(ages, train_pred))
        
        print(f"   è®­ç»ƒMAE: {train_mae:.3f} å²")
        print(f"   è®­ç»ƒRMSE: {train_rmse:.3f} å²")
        
        return self
    
    def predict(self, features: np.ndarray) -> np.ndarray:
        """é¢„æµ‹å¹´é¾„"""
        if not self.is_fitted:
            raise ValueError("æ¨¡åž‹æœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨fitæ–¹æ³•")
        
        # PCAå˜æ¢
        if self.use_pca:
            features_transformed = self.pca.transform(features)
        else:
            features_transformed = features
        
        # é¢„æµ‹
        predictions = self.regressor.predict(features_transformed)
        return predictions
    
    def get_feature_names(self) -> List[str]:
        """èŽ·å–ç‰¹å¾åç§°"""
        if self.use_pca:
            return [f'PC{i+1}' for i in range(self.n_components)]
        else:
            return [f'ç‰¹å¾_{i+1}' for i in range(self.regressor.n_features_in_)]

def create_real_data_results_table(data_path: str = "data", 
                                 max_samples: int = 150,
                                 test_size: float = 0.3) -> pd.DataFrame:
    """
    ä½¿ç”¨çœŸå®žæ•°æ®åˆ›å»ºç»“æžœè¡¨æ ¼
    
    Args:
        data_path: æ•°æ®è·¯å¾„
        max_samples: æœ€å¤§æ ·æœ¬æ•°
        test_size: æµ‹è¯•é›†æ¯”ä¾‹
        
    Returns:
        pd.DataFrame: ç»“æžœè¡¨æ ¼
    """
    print("ðŸš€ å¼€å§‹å¤„ç†çœŸå®žUTKFaceæ•°æ®...")
    
    # 1. åŠ è½½æ•°æ®
    processor = UTKFaceRealDataProcessor(data_path)
    features, ages, filenames, sample_ids = processor.load_sample_data(max_samples)
    
    print(f"\nðŸ“Š æ•°æ®é›†ä¿¡æ¯:")
    print(f"   æ ·æœ¬æ•°é‡: {len(features)}")
    print(f"   ç‰¹å¾ç»´åº¦: {features.shape[1]}")
    print(f"   å¹´é¾„èŒƒå›´: {ages.min()} - {ages.max()} å²")
    print(f"   å¹³å‡å¹´é¾„: {ages.mean():.1f} å²")
    
    # 2. åˆ’åˆ†è®­ç»ƒæµ‹è¯•é›†
    if len(features) > 10:  # è‡³å°‘éœ€è¦10ä¸ªæ ·æœ¬æ‰èƒ½åˆ’åˆ†
        X_train, X_test, y_train, y_test, idx_train, idx_test, files_train, files_test = train_test_split(
            features, ages, sample_ids, filenames, test_size=test_size, random_state=42
        )
        print(f"\nðŸ“Š æ•°æ®åˆ’åˆ†:")
        print(f"   è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
        print(f"   æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")
    else:
        # æ ·æœ¬å¤ªå°‘ï¼Œå…¨éƒ¨ç”¨ä½œæµ‹è¯•
        X_train, X_test = features, features
        y_train, y_test = ages, ages
        idx_train, idx_test = sample_ids, sample_ids
        files_train, files_test = filenames, filenames
        print(f"\nâš ï¸  æ ·æœ¬æ•°é‡è¾ƒå°‘ï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®")
    
    # 3. è®­ç»ƒé¢„æµ‹æ¨¡åž‹
    predictor = AgePredictor(use_pca=True, n_components=min(10, features.shape[1]))
    predictor.fit(X_train, y_train)
    
    # 4. åœ¨æµ‹è¯•é›†ä¸Šé¢„æµ‹
    print(f"\nðŸŽ¯ åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œé¢„æµ‹...")
    predictions = predictor.predict(X_test)
    
    # 5. è®¡ç®—è¯¯å·®
    abs_errors = np.abs(predictions - y_test)
    rel_errors = (abs_errors / np.maximum(y_test, 1e-6)) * 100
    
    mae = mean_absolute_error(y_test, predictions)
    rmse = np.sqrt(mean_squared_error(y_test, predictions))
    
    print(f"ðŸ“ˆ æµ‹è¯•é›†æ€§èƒ½:")
    print(f"   MAE: {mae:.3f} å²")
    print(f"   RMSE: {rmse:.3f} å²")
    print(f"   å¹³å‡ç›¸å¯¹è¯¯å·®: {rel_errors.mean():.1f}%")
    
    # 6. èŽ·å–ç‰¹å¾å€¼ï¼ˆæµ‹è¯•é›†çš„PCAå˜æ¢ç»“æžœï¼‰
    if predictor.use_pca:
        test_features_transformed = predictor.pca.transform(X_test)
    else:
        test_features_transformed = X_test
    
    # 7. åˆ›å»ºç»“æžœè¡¨æ ¼
    print(f"\nðŸ“‹ åˆ›å»ºç»“æžœè¡¨æ ¼...")
    
    # æž„å»ºè¡¨æ ¼æ•°æ®
    table_data = {
        'æ ·æœ¬ID': idx_test,
        'æ–‡ä»¶å': files_test,
    }
    
    # æ·»åŠ ç‰¹å¾åˆ—
    feature_names = predictor.get_feature_names()
    for i, feature_name in enumerate(feature_names):
        table_data[feature_name] = test_features_transformed[:, i]
    
    # æ·»åŠ é¢„æµ‹ç»“æžœåˆ—
    table_data['é¢„æµ‹å€¼'] = predictions
    table_data['çœŸå®žå€¼'] = y_test
    table_data['ç»å¯¹è¯¯å·®'] = abs_errors
    table_data['ç›¸å¯¹è¯¯å·®(%)'] = rel_errors
    
    # åˆ›å»ºDataFrame
    results_df = pd.DataFrame(table_data)
    
    # æŒ‰ç»å¯¹è¯¯å·®æŽ’åºï¼Œå±•ç¤ºä¸åŒè¯¯å·®æ°´å¹³çš„æ ·æœ¬
    results_df = results_df.sort_values('ç»å¯¹è¯¯å·®').reset_index(drop=True)
    
    return results_df

def save_results_to_csv(df: pd.DataFrame, 
                       csv_path: str = 'results/metrics/utkface_real_results.csv',
                       summary_path: str = 'results/metrics/utkface_summary.csv'):
    """
    ä¿å­˜ç»“æžœåˆ°CSVæ–‡ä»¶
    
    Args:
        df: ç»“æžœDataFrame
        csv_path: è¯¦ç»†ç»“æžœCSVè·¯å¾„
        summary_path: ç»Ÿè®¡æ‘˜è¦CSVè·¯å¾„
    """
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(os.path.dirname(csv_path), exist_ok=True)
    
    # 1. ä¿å­˜è¯¦ç»†ç»“æžœ
    df.to_csv(csv_path, index=False, encoding='utf-8-sig')
    print(f"âœ… è¯¦ç»†ç»“æžœå·²ä¿å­˜åˆ°: {csv_path}")
    
    # 2. åˆ›å»ºç»Ÿè®¡æ‘˜è¦
    abs_errors = df['ç»å¯¹è¯¯å·®'].values
    
    # è¯¯å·®åŒºé—´ç»Ÿè®¡
    error_bins = [0, 1, 2, 3, 5, 10, float('inf')]
    error_labels = ['0-1å²', '1-2å²', '2-3å²', '3-5å²', '5-10å²', '>10å²']
    
    error_stats = []
    cumulative_percentage = 0
    
    for i in range(len(error_bins)-1):
        mask = (abs_errors >= error_bins[i]) & (abs_errors < error_bins[i+1])
        count = np.sum(mask)
        percentage = count / len(abs_errors) * 100
        cumulative_percentage += percentage
        
        error_stats.append({
            'è¯¯å·®åŒºé—´': error_labels[i],
            'æ ·æœ¬æ•°é‡': count,
            'å æ¯”(%)': f'{percentage:.1f}%',
            'ç´¯è®¡å æ¯”(%)': f'{cumulative_percentage:.1f}%'
        })
    
    # è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
    detailed_stats = {
        'ç»Ÿè®¡æŒ‡æ ‡': [
            'æ ·æœ¬æ€»æ•°', 'å¹³å‡ç»å¯¹è¯¯å·®', 'è¯¯å·®æ ‡å‡†å·®', 'ä¸­ä½æ•°è¯¯å·®',
            'æœ€å°è¯¯å·®', 'æœ€å¤§è¯¯å·®', '25%åˆ†ä½æ•°', '75%åˆ†ä½æ•°',
            'å¹³å‡ç›¸å¯¹è¯¯å·®(%)', 'RMSE', 'é¢„æµ‹å€¼å‡å€¼', 'çœŸå®žå€¼å‡å€¼'
        ],
        'æ•°å€¼': [
            len(df),
            f'{df["ç»å¯¹è¯¯å·®"].mean():.3f}',
            f'{df["ç»å¯¹è¯¯å·®"].std():.3f}',
            f'{df["ç»å¯¹è¯¯å·®"].median():.3f}',
            f'{df["ç»å¯¹è¯¯å·®"].min():.3f}',
            f'{df["ç»å¯¹è¯¯å·®"].max():.3f}',
            f'{df["ç»å¯¹è¯¯å·®"].quantile(0.25):.3f}',
            f'{df["ç»å¯¹è¯¯å·®"].quantile(0.75):.3f}',
            f'{df["ç›¸å¯¹è¯¯å·®(%)"].mean():.1f}%',
            f'{np.sqrt(np.mean((df["é¢„æµ‹å€¼"] - df["çœŸå®žå€¼"])**2)):.3f}',
            f'{df["é¢„æµ‹å€¼"].mean():.1f}',
            f'{df["çœŸå®žå€¼"].mean():.1f}'
        ]
    }
    
    # åˆå¹¶ç»Ÿè®¡æ•°æ®
    summary_data = []
    
    # æ·»åŠ è¯¯å·®åˆ†å¸ƒç»Ÿè®¡
    for stat in error_stats:
        summary_data.append(stat)
    
    # æ·»åŠ åˆ†éš”è¡Œ
    summary_data.append({'è¯¯å·®åŒºé—´': '---è¯¦ç»†ç»Ÿè®¡---', 'æ ·æœ¬æ•°é‡': '', 'å æ¯”(%)': '', 'ç´¯è®¡å æ¯”(%)': ''})
    
    # æ·»åŠ è¯¦ç»†ç»Ÿè®¡
    for i, metric in enumerate(detailed_stats['ç»Ÿè®¡æŒ‡æ ‡']):
        summary_data.append({
            'è¯¯å·®åŒºé—´': metric,
            'æ ·æœ¬æ•°é‡': detailed_stats['æ•°å€¼'][i],
            'å æ¯”(%)': '',
            'ç´¯è®¡å æ¯”(%)': ''
        })
    
    summary_df = pd.DataFrame(summary_data)
    summary_df.to_csv(summary_path, index=False, encoding='utf-8-sig')
    print(f"âœ… ç»Ÿè®¡æ‘˜è¦å·²ä¿å­˜åˆ°: {summary_path}")
    
    # 3. æ˜¾ç¤ºç»“æžœé¢„è§ˆ
    print(f"\nðŸ“‹ ç»“æžœè¡¨æ ¼é¢„è§ˆ (å‰10è¡Œ):")
    display_columns = ['æ ·æœ¬ID', 'æ–‡ä»¶å', 'é¢„æµ‹å€¼', 'çœŸå®žå€¼', 'ç»å¯¹è¯¯å·®', 'ç›¸å¯¹è¯¯å·®(%)']
    if 'PC1' in df.columns:
        display_columns.insert(2, 'PC1')
        display_columns.insert(3, 'PC2')
    
    print(df[display_columns].head(10).to_string(index=False))
    
    print(f"\nðŸ“Š æ•°æ®æ¦‚è§ˆ:")
    print(f"   æ€»æ ·æœ¬æ•°: {len(df)}")
    print(f"   å¹³å‡ç»å¯¹è¯¯å·®: {df['ç»å¯¹è¯¯å·®'].mean():.3f} å²")
    print(f"   è¯¯å·®æ ‡å‡†å·®: {df['ç»å¯¹è¯¯å·®'].std():.3f} å²")
    print(f"   æœ€å¤§è¯¯å·®: {df['ç»å¯¹è¯¯å·®'].max():.3f} å²")
    print(f"   æœ€å°è¯¯å·®: {df['ç»å¯¹è¯¯å·®'].min():.3f} å²")

def main():
    """ä¸»å‡½æ•°"""
    print("ðŸŽ¯ UTKFaceçœŸå®žæ•°æ®ç»“æžœè¡¨æ ¼ç”Ÿæˆ")
    print("=" * 50)
    
    # æ£€æŸ¥æ•°æ®ç›®å½•
    data_path = "data"
    if not os.path.exists(data_path):
        print(f"âš ï¸  æ•°æ®ç›®å½• {data_path} ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºå¹¶ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
        os.makedirs(data_path, exist_ok=True)
    
    try:
        # åˆ›å»ºç»“æžœè¡¨æ ¼
        results_df = create_real_data_results_table(
            data_path=data_path,
            max_samples=200,  # å¤„ç†æœ€å¤š200ä¸ªæ ·æœ¬
            test_size=0.3     # 30%ä½œä¸ºæµ‹è¯•é›†
        )
        
        # ä¿å­˜åˆ°CSV
        save_results_to_csv(
            df=results_df,
            csv_path='results/metrics/utkface_real_results.csv',
            summary_path='results/metrics/utkface_summary.csv'
        )
        
        print(f"\nðŸŽ‰ å¤„ç†å®Œæˆï¼")
        print(f"ðŸ“ ç»“æžœæ–‡ä»¶:")
        print(f"   - è¯¦ç»†ç»“æžœ: results/metrics/utkface_real_results.csv")
        print(f"   - ç»Ÿè®¡æ‘˜è¦: results/metrics/utkface_summary.csv")
        
    except Exception as e:
        print(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºçŽ°é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 