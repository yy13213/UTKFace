#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
100%çœŸå®UTKFaceæ•°æ®CSVç”Ÿæˆå™¨
åªä½¿ç”¨çœŸå®çš„UTKFaceæ•°æ®é›†ï¼Œä¸åŒ…å«ä»»ä½•æ¨¡æ‹Ÿæ•°æ®
æ ¼å¼ï¼šç‰¹å¾åˆ— | é¢„æµ‹å€¼ | çœŸå®å€¼ | ç»å¯¹è¯¯å·®
"""

import os
import sys
import requests
import zipfile
import tarfile
import gdown
import numpy as np
import pandas as pd
import torch
import torchvision.transforms as transforms
from PIL import Image
import glob
from pathlib import Path
from typing import Tuple, List, Optional
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.preprocessing import StandardScaler
import cv2
import warnings
warnings.filterwarnings('ignore')

class RealUTKFaceOnlyDownloader:
    """100%çœŸå®UTKFaceæ•°æ®ä¸‹è½½å™¨"""
    
    def __init__(self, data_dir: str = "data"):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)
        
    def check_existing_data(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å·²æœ‰çœŸå®UTKFaceæ•°æ®"""
        print("ğŸ” æ£€æŸ¥ç°æœ‰æ•°æ®...")
        
        # æœç´¢æ‰€æœ‰å¯èƒ½çš„å›¾åƒæ–‡ä»¶ä½ç½®
        search_patterns = [
            self.data_dir / "*.jpg",
            self.data_dir / "**/*.jpg", 
            self.data_dir / "UTKFace" / "*.jpg",
            self.data_dir / "utkface-new" / "*.jpg",
            self.data_dir / "crop_part1" / "*.jpg",
        ]
        
        all_images = []
        for pattern in search_patterns:
            images = list(Path().glob(str(pattern)))
            all_images.extend(images)
        
        # éªŒè¯æ˜¯å¦ä¸ºçœŸå®UTKFaceæ ¼å¼çš„æ–‡ä»¶
        valid_images = []
        for img_path in all_images:
            if self.is_valid_utkface_filename(img_path.name):
                valid_images.append(img_path)
        
        if len(valid_images) > 50:  # è‡³å°‘éœ€è¦50ä¸ªçœŸå®å›¾åƒ
            print(f"âœ… å‘ç° {len(valid_images)} ä¸ªçœŸå®UTKFaceå›¾åƒ")
            return True
        else:
            print(f"âŒ åªæ‰¾åˆ° {len(valid_images)} ä¸ªæœ‰æ•ˆå›¾åƒï¼Œéœ€è¦æ›´å¤šçœŸå®æ•°æ®")
            return False
    
    def is_valid_utkface_filename(self, filename: str) -> bool:
        """éªŒè¯æ˜¯å¦ä¸ºçœŸå®UTKFaceæ–‡ä»¶åæ ¼å¼"""
        try:
            name = os.path.splitext(filename)[0]
            parts = name.split('_')
            
            # UTKFaceæ ¼å¼: [age]_[gender]_[race]_[date&time].jpg
            if len(parts) >= 4:
                age = int(parts[0])
                gender = int(parts[1])
                race = int(parts[2])
                
                # éªŒè¯èŒƒå›´
                if 0 <= age <= 120 and 0 <= gender <= 1 and 0 <= race <= 4:
                    return True
            return False
        except (ValueError, IndexError):
            return False
    
    def download_utkface_multiple_sources(self) -> bool:
        """ä»å¤šä¸ªæºä¸‹è½½çœŸå®UTKFaceæ•°æ®"""
        print("ğŸš€ å°è¯•ä»å¤šä¸ªæºä¸‹è½½çœŸå®UTKFaceæ•°æ®é›†...")
        
        # ä¸‹è½½æºåˆ—è¡¨
        download_sources = [
            {
                "name": "Google Drive - UTKFace Aligned",
                "method": "gdown",
                "file_id": "0BxYys69jI14kYVM3aVhKS1VhRUk",
                "filename": "UTKFace.tar.gz"
            },
            {
                "name": "Dropbox Mirror",
                "method": "direct",
                "url": "https://www.dropbox.com/s/bg5n8bk8kjxddx5/UTKFace.tar.gz?dl=1",
                "filename": "UTKFace.tar.gz"
            },
            {
                "name": "Archive.org Mirror",
                "method": "direct", 
                "url": "https://archive.org/download/utkface/UTKFace.tar.gz",
                "filename": "UTKFace.tar.gz"
            }
        ]
        
        for source in download_sources:
            print(f"\nğŸ”„ å°è¯•ä» {source['name']} ä¸‹è½½...")
            
            try:
                if source['method'] == 'gdown':
                    success = self._download_via_gdown(source['file_id'], source['filename'])
                elif source['method'] == 'direct':
                    success = self._download_direct(source['url'], source['filename'])
                
                if success:
                    print(f"âœ… ä» {source['name']} ä¸‹è½½æˆåŠŸ!")
                    return True
                    
            except Exception as e:
                print(f"âŒ ä» {source['name']} ä¸‹è½½å¤±è´¥: {str(e)}")
                continue
        
        return False
    
    def _download_via_gdown(self, file_id: str, filename: str) -> bool:
        """é€šè¿‡gdownä»Google Driveä¸‹è½½"""
        try:
            output_path = self.data_dir / filename
            url = f"https://drive.google.com/uc?id={file_id}"
            
            print(f"   ğŸ“¥ ä»Google Driveä¸‹è½½: {filename}")
            gdown.download(url, str(output_path), quiet=False)
            
            if output_path.exists() and output_path.stat().st_size > 1024*1024:  # è‡³å°‘1MB
                return self._extract_archive(output_path)
            return False
            
        except Exception as e:
            print(f"   âŒ gdownä¸‹è½½å¤±è´¥: {str(e)}")
            return False
    
    def _download_direct(self, url: str, filename: str) -> bool:
        """ç›´æ¥HTTPä¸‹è½½"""
        try:
            output_path = self.data_dir / filename
            
            print(f"   ğŸ“¥ ç›´æ¥ä¸‹è½½: {filename}")
            response = requests.get(url, stream=True, timeout=30)
            response.raise_for_status()
            
            total_size = int(response.headers.get('content-length', 0))
            
            with open(output_path, 'wb') as f:
                downloaded = 0
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        downloaded += len(chunk)
                        
                        if total_size > 0:
                            progress = (downloaded / total_size) * 100
                            print(f"\r   è¿›åº¦: {progress:.1f}%", end="", flush=True)
            
            print(f"\n   âœ… ä¸‹è½½å®Œæˆ: {output_path}")
            
            if output_path.exists() and output_path.stat().st_size > 1024*1024:  # è‡³å°‘1MB
                return self._extract_archive(output_path)
            return False
            
        except Exception as e:
            print(f"   âŒ ç›´æ¥ä¸‹è½½å¤±è´¥: {str(e)}")
            return False
    
    def _extract_archive(self, archive_path: Path) -> bool:
        """è§£å‹å½’æ¡£æ–‡ä»¶"""
        print(f"ğŸ“¦ è§£å‹æ–‡ä»¶: {archive_path.name}")
        
        try:
            if archive_path.suffix == '.zip':
                with zipfile.ZipFile(archive_path, 'r') as zip_ref:
                    zip_ref.extractall(self.data_dir)
                    
            elif archive_path.suffix in ['.tar', '.gz'] or 'tar.gz' in archive_path.name:
                with tarfile.open(archive_path, 'r:*') as tar_ref:
                    tar_ref.extractall(self.data_dir)
            
            # æ£€æŸ¥è§£å‹ç»“æœ
            jpg_files = list(self.data_dir.glob("*.jpg")) + list(self.data_dir.glob("**/*.jpg"))
            valid_files = [f for f in jpg_files if self.is_valid_utkface_filename(f.name)]
            
            if len(valid_files) > 50:
                print(f"   âœ… è§£å‹æˆåŠŸ: æ‰¾åˆ° {len(valid_files)} ä¸ªæœ‰æ•ˆUTKFaceå›¾åƒ")
                # åˆ é™¤å½’æ¡£æ–‡ä»¶ä»¥èŠ‚çœç©ºé—´
                archive_path.unlink()
                return True
            else:
                print(f"   âŒ è§£å‹ååªæ‰¾åˆ° {len(valid_files)} ä¸ªæœ‰æ•ˆå›¾åƒ")
                return False
                
        except Exception as e:
            print(f"   âŒ è§£å‹å¤±è´¥: {str(e)}")
            return False
    
    def get_real_data(self) -> bool:
        """è·å–çœŸå®UTKFaceæ•°æ®"""
        print("ğŸ¯ è·å–100%çœŸå®UTKFaceæ•°æ®é›†")
        print("=" * 60)
        
        # 1. æ£€æŸ¥ç°æœ‰æ•°æ®
        if self.check_existing_data():
            return True
        
        # 2. å°è¯•ä¸‹è½½çœŸå®æ•°æ®
        if self.download_utkface_multiple_sources():
            return self.check_existing_data()
        
        # 3. æä¾›æ‰‹åŠ¨ä¸‹è½½æŒ‡å¯¼
        print("\nâŒ è‡ªåŠ¨ä¸‹è½½å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨ä¸‹è½½çœŸå®UTKFaceæ•°æ®é›†:")
        print("ğŸ“ æ‰‹åŠ¨ä¸‹è½½æŒ‡å—:")
        print("   1. è®¿é—®å®˜æ–¹ç½‘ç«™: https://susanqq.github.io/UTKFace/")
        print("   2. æˆ–è®¿é—®Kaggle: https://www.kaggle.com/datasets/jangedoo/utkface-new")
        print("   3. ä¸‹è½½UTKFaceæ•°æ®é›†")
        print("   4. è§£å‹åˆ°å½“å‰ç›®å½•çš„ data/ æ–‡ä»¶å¤¹")
        print("   5. ç¡®ä¿å›¾åƒæ–‡ä»¶æ ¼å¼ä¸º: [age]_[gender]_[race]_[timestamp].jpg")
        print("   6. é‡æ–°è¿è¡Œæ­¤è„šæœ¬")
        print("\nğŸ’¡ æç¤º: æ‚¨ä¹Ÿå¯ä»¥å°†UTKFaceå›¾åƒæ–‡ä»¶ç›´æ¥æ”¾å…¥data/ç›®å½•")
        
        return False

class RealUTKFaceProcessor:
    """100%çœŸå®UTKFaceæ•°æ®å¤„ç†å™¨"""
    
    def __init__(self, data_dir: str = "data"):
        self.data_dir = Path(data_dir)
        self.transform = transforms.Compose([
            transforms.Resize((128, 128)),
            transforms.ToTensor()
        ])
    
    def parse_utkface_filename(self, filename: str) -> Optional[dict]:
        """è§£æUTKFaceæ–‡ä»¶å"""
        try:
            name = os.path.splitext(filename)[0]
            parts = name.split('_')
            
            if len(parts) >= 4:
                return {
                    'age': int(parts[0]),
                    'gender': int(parts[1]), 
                    'race': int(parts[2]),
                    'timestamp': parts[3] if len(parts) > 3 else '0'
                }
            return None
        except (ValueError, IndexError):
            return None
    
    def extract_real_features(self, image_path: Path) -> Optional[np.ndarray]:
        """ä»çœŸå®å›¾åƒä¸­æå–30ç»´ç‰¹å¾"""
        try:
            # åŠ è½½å›¾åƒ
            image = Image.open(image_path).convert('RGB')
            tensor = self.transform(image)
            img_array = tensor.numpy()
            
            features = []
            
            # RGBé€šé“ç»Ÿè®¡ç‰¹å¾ (21ç»´)
            for channel in range(3):
                channel_data = img_array[channel].flatten()
                channel_features = [
                    np.mean(channel_data),      # å‡å€¼
                    np.std(channel_data),       # æ ‡å‡†å·®
                    np.median(channel_data),    # ä¸­ä½æ•°
                    np.percentile(channel_data, 25),  # 25%åˆ†ä½æ•°
                    np.percentile(channel_data, 75),  # 75%åˆ†ä½æ•°
                    np.min(channel_data),       # æœ€å°å€¼
                    np.max(channel_data),       # æœ€å¤§å€¼
                ]
                features.extend(channel_features)
            
            # å…¨å±€ç»Ÿè®¡ç‰¹å¾ (5ç»´)
            all_pixels = img_array.flatten()
            global_features = [
                np.mean(all_pixels),                    # å…¨å±€å‡å€¼
                np.std(all_pixels),                     # å…¨å±€æ ‡å‡†å·®
                np.var(all_pixels),                     # å…¨å±€æ–¹å·®
                np.sum(all_pixels > np.mean(all_pixels)), # é«˜äºå‡å€¼çš„åƒç´ æ•°
                np.sum(all_pixels < np.mean(all_pixels)), # ä½äºå‡å€¼çš„åƒç´ æ•°
            ]
            features.extend(global_features)
            
            # çº¹ç†ç‰¹å¾ (4ç»´)
            gray = np.mean(img_array, axis=0)
            
            # è®¡ç®—æ¢¯åº¦
            grad_x = np.diff(gray, axis=1)
            grad_y = np.diff(gray, axis=0)
            
            texture_features = [
                np.mean(np.abs(grad_x)),    # Xæ–¹å‘æ¢¯åº¦å‡å€¼
                np.mean(np.abs(grad_y)),    # Yæ–¹å‘æ¢¯åº¦å‡å€¼
                np.std(grad_x),             # Xæ–¹å‘æ¢¯åº¦æ ‡å‡†å·®
                np.std(grad_y),             # Yæ–¹å‘æ¢¯åº¦æ ‡å‡†å·®
            ]
            features.extend(texture_features)
            
            return np.array(features)
            
        except Exception as e:
            print(f"   âŒ ç‰¹å¾æå–å¤±è´¥ {image_path.name}: {str(e)}")
            return None
    
    def load_real_dataset(self, max_samples: int = 1000) -> Tuple[np.ndarray, np.ndarray, List[str]]:
        """åŠ è½½100%çœŸå®UTKFaceæ•°æ®é›†"""
        print(f"ğŸ“‚ åŠ è½½çœŸå®UTKFaceæ•°æ®é›†...")
        
        # æœç´¢æ‰€æœ‰çœŸå®UTKFaceå›¾åƒ
        image_patterns = [
            self.data_dir / "*.jpg",
            self.data_dir / "**/*.jpg",
            self.data_dir / "UTKFace" / "*.jpg",
            self.data_dir / "crop_part1" / "*.jpg",
        ]
        
        all_images = []
        for pattern in image_patterns:
            images = list(Path().glob(str(pattern)))
            all_images.extend(images)
        
        # è¿‡æ»¤å‡ºæœ‰æ•ˆçš„UTKFaceæ–‡ä»¶
        valid_images = []
        for img_path in all_images:
            info = self.parse_utkface_filename(img_path.name)
            if info and 0 <= info['age'] <= 120:
                valid_images.append((img_path, info))
        
        if len(valid_images) == 0:
            raise ValueError("âŒ æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„çœŸå®UTKFaceå›¾åƒæ–‡ä»¶!")
        
        print(f"   âœ… æ‰¾åˆ° {len(valid_images)} ä¸ªæœ‰æ•ˆçš„çœŸå®UTKFaceå›¾åƒ")
        
        # é™åˆ¶æ ·æœ¬æ•°é‡
        if len(valid_images) > max_samples:
            valid_images = valid_images[:max_samples]
            print(f"   ğŸ“Š é™åˆ¶ä½¿ç”¨å‰ {max_samples} ä¸ªæ ·æœ¬")
        
        # æå–ç‰¹å¾å’Œæ ‡ç­¾
        features_list = []
        ages_list = []
        filenames_list = []
        
        print(f"   ğŸ”„ æ­£åœ¨æå–ç‰¹å¾...")
        processed = 0
        
        for img_path, info in valid_images:
            features = self.extract_real_features(img_path)
            
            if features is not None:
                features_list.append(features)
                ages_list.append(info['age'])
                filenames_list.append(img_path.name)
                processed += 1
                
                if processed % 50 == 0:
                    print(f"   ğŸ“Š å·²å¤„ç†: {processed}/{len(valid_images)}")
        
        if len(features_list) == 0:
            raise ValueError("âŒ æ— æ³•ä»ä»»ä½•å›¾åƒä¸­æå–æœ‰æ•ˆç‰¹å¾!")
        
        print(f"   âœ… æˆåŠŸå¤„ç† {len(features_list)} ä¸ªçœŸå®æ ·æœ¬")
        print(f"   ğŸ“Š å¹´é¾„èŒƒå›´: {min(ages_list)}-{max(ages_list)} å²")
        print(f"   ğŸ“Š å¹³å‡å¹´é¾„: {np.mean(ages_list):.1f} å²")
        
        return np.array(features_list), np.array(ages_list), filenames_list

def create_real_utkface_csv_only() -> pd.DataFrame:
    """åˆ›å»º100%åŸºäºçœŸå®UTKFaceæ•°æ®çš„CSVè¡¨æ ¼"""
    
    print("ğŸ¯ 100%çœŸå®UTKFaceæ•°æ®CSVè¡¨æ ¼ç”Ÿæˆå™¨")
    print("=" * 70)
    print("ğŸ“‹ ç‰¹ç‚¹:")
    print("   âœ… 100%ä½¿ç”¨çœŸå®UTKFaceå›¾åƒæ•°æ®")
    print("   âœ… ä¸åŒ…å«ä»»ä½•æ¨¡æ‹Ÿæˆ–ç”Ÿæˆæ•°æ®")
    print("   âœ… ç›´æ¥ä»çœŸå®é¢éƒ¨å›¾åƒæå–ç‰¹å¾")
    print("   âœ… ä½¿ç”¨çœŸå®çš„å¹´é¾„æ ‡ç­¾")
    print("=" * 70)
    
    # 1. ä¸‹è½½/æ£€æŸ¥çœŸå®æ•°æ®
    downloader = RealUTKFaceOnlyDownloader("data")
    if not downloader.get_real_data():
        raise ValueError("æ— æ³•è·å–çœŸå®UTKFaceæ•°æ®é›†ï¼Œè¯·æ‰‹åŠ¨ä¸‹è½½åé‡è¯•")
    
    # 2. åŠ è½½çœŸå®æ•°æ®é›†
    processor = RealUTKFaceProcessor("data")
    features, ages, filenames = processor.load_real_dataset(max_samples=1000)
    
    print(f"\nğŸ“Š çœŸå®æ•°æ®é›†ç»Ÿè®¡:")
    print(f"   æ ·æœ¬æ•°é‡: {len(features)}")
    print(f"   ç‰¹å¾ç»´åº¦: {features.shape[1]}")
    print(f"   å¹´é¾„åˆ†å¸ƒ:")
    print(f"     18-30å²: {np.sum((ages >= 18) & (ages <= 30))} ä¸ª")
    print(f"     31-50å²: {np.sum((ages >= 31) & (ages <= 50))} ä¸ª") 
    print(f"     51-70å²: {np.sum((ages >= 51) & (ages <= 70))} ä¸ª")
    print(f"     70+å²: {np.sum(ages > 70)} ä¸ª")
    
    # 3. ç‰¹å¾æ ‡å‡†åŒ–
    print(f"\nğŸ”§ æ•°æ®é¢„å¤„ç†...")
    scaler = StandardScaler()
    features_scaled = scaler.fit_transform(features)
    
    # 4. æ•°æ®åˆ’åˆ†
    test_size = min(0.3, 200/len(features))  # æœ€å¤š30%æˆ–200ä¸ªæ ·æœ¬ä½œä¸ºæµ‹è¯•é›†
    X_train, X_test, y_train, y_test, files_train, files_test = train_test_split(
        features_scaled, ages, filenames, test_size=test_size, random_state=42
    )
    
    print(f"   è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
    print(f"   æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")
    
    # 5. è®­ç»ƒå¹´é¾„é¢„æµ‹æ¨¡å‹
    print(f"\nğŸ¯ è®­ç»ƒå¹´é¾„é¢„æµ‹æ¨¡å‹...")
    model = RandomForestRegressor(
        n_estimators=200,
        max_depth=15,
        random_state=42,
        min_samples_split=5,
        min_samples_leaf=2,
        max_features='sqrt'
    )
    model.fit(X_train, y_train)
    
    # 6. é¢„æµ‹å’Œè¯„ä¼°
    test_pred = model.predict(X_test)
    train_pred = model.predict(X_train)
    
    test_mae = mean_absolute_error(y_test, test_pred)
    test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))
    train_mae = mean_absolute_error(y_train, train_pred)
    correlation = np.corrcoef(y_test, test_pred)[0,1]
    
    print(f"\nğŸ“ˆ æ¨¡å‹æ€§èƒ½ (100%çœŸå®æ•°æ®):")
    print(f"   æµ‹è¯•é›† MAE: {test_mae:.3f} å²")
    print(f"   æµ‹è¯•é›† RMSE: {test_rmse:.3f} å²")
    print(f"   è®­ç»ƒé›† MAE: {train_mae:.3f} å²")
    print(f"   ç›¸å…³ç³»æ•°: {correlation:.3f}")
    
    # 7. åˆ›å»ºCSVè¡¨æ ¼
    print(f"\nğŸ“‹ åˆ›å»ºCSVè¡¨æ ¼...")
    
    # ç”Ÿæˆç‰¹å¾åˆ—å
    feature_names = []
    for channel in ['R', 'G', 'B']:
        for stat in ['mean', 'std', 'median', 'q25', 'q75', 'min', 'max']:
            feature_names.append(f'{channel}_{stat}')
    
    for stat in ['global_mean', 'global_std', 'global_var', 'bright_pixels', 'dark_pixels']:
        feature_names.append(stat)
    
    for stat in ['grad_x_mean', 'grad_y_mean', 'grad_x_std', 'grad_y_std']:
        feature_names.append(stat)
    
    # æ„å»ºè¡¨æ ¼æ•°æ®
    table_data = {}
    
    # æ·»åŠ 30ç»´ç‰¹å¾
    for i, feature_name in enumerate(feature_names):
        table_data[feature_name] = X_test[:, i]
    
    # æ·»åŠ é¢„æµ‹ç»“æœ
    abs_errors = np.abs(test_pred - y_test)
    table_data['Predicted_Age'] = np.round(test_pred, 2)
    table_data['Actual_Age'] = y_test
    table_data['Abs_Error'] = np.round(abs_errors, 2)
    table_data['Filename'] = files_test
    
    # åˆ›å»ºDataFrameå¹¶æŒ‰è¯¯å·®æ’åº
    df = pd.DataFrame(table_data)
    df = df.sort_values('Abs_Error').reset_index(drop=True)
    
    print(f"âœ… CSVè¡¨æ ¼åˆ›å»ºå®Œæˆ")
    print(f"   è¡Œæ•°: {len(df)} (å…¨éƒ¨ä¸ºçœŸå®æµ‹è¯•æ ·æœ¬)")
    print(f"   åˆ—æ•°: {len(df.columns)}")
    print(f"   æ•°æ®æ¥æº: 100%çœŸå®UTKFaceå›¾åƒ")
    
    return df

def main():
    """ä¸»å‡½æ•°"""
    try:
        # ç”ŸæˆåŸºäº100%çœŸå®æ•°æ®çš„CSVè¡¨æ ¼
        results_df = create_real_utkface_csv_only()
        
        # ä¿å­˜ç»“æœ 
        output_dir = Path('results/metrics')
        output_dir.mkdir(parents=True, exist_ok=True)
        output_path = output_dir / 'real_only_utkface_features.csv'
        
        # é‡æ–°æ’åˆ—åˆ—çš„é¡ºåºï¼š30ç»´ç‰¹å¾åœ¨å‰ï¼Œç»“æœåœ¨å
        feature_cols = [col for col in results_df.columns if col not in ['Predicted_Age', 'Actual_Age', 'Abs_Error', 'Filename']]
        result_cols = ['Predicted_Age', 'Actual_Age', 'Abs_Error']
        final_cols = feature_cols + result_cols
        
        final_df = results_df[final_cols].copy()
        final_df.to_csv(output_path, index=False, encoding='utf-8')
        
        print(f"\nğŸ’¾ 100%çœŸå®æ•°æ®CSVå·²ä¿å­˜: {output_path}")
        
        # æ˜¾ç¤ºç»“æœé¢„è§ˆ
        print(f"\nğŸ“‹ çœŸå®æ•°æ®è¡¨æ ¼é¢„è§ˆ:")
        preview_cols = feature_cols[:5] + result_cols
        print(final_df[preview_cols].head().to_string(index=False, float_format='%.3f'))
        
        # æ€§èƒ½ç»Ÿè®¡
        print(f"\nğŸ“Š çœŸå®æ•°æ®æ€§èƒ½ç»Ÿè®¡:")
        abs_errors = final_df['Abs_Error']
        print(f"   æ ·æœ¬æ€»æ•°: {len(final_df)} (100%çœŸå®)")
        print(f"   å¹³å‡ç»å¯¹è¯¯å·®: {abs_errors.mean():.3f} å²")
        print(f"   ä¸­ä½æ•°è¯¯å·®: {abs_errors.median():.3f} å²")
        print(f"   æœ€å¤§è¯¯å·®: {abs_errors.max():.3f} å²")
        print(f"   æœ€å°è¯¯å·®: {abs_errors.min():.3f} å²")
        
        # è¯¯å·®åˆ†å¸ƒåˆ†æ
        excellent = np.sum(abs_errors <= 3)
        good = np.sum((abs_errors > 3) & (abs_errors <= 6))
        fair = np.sum((abs_errors > 6) & (abs_errors <= 10))
        poor = np.sum(abs_errors > 10)
        
        print(f"\nğŸ¯ çœŸå®æ•°æ®è¯¯å·®åˆ†å¸ƒ:")
        print(f"   ä¼˜ç§€ (â‰¤3å²): {excellent} ä¸ª ({excellent/len(abs_errors)*100:.1f}%)")
        print(f"   è‰¯å¥½ (3-6å²): {good} ä¸ª ({good/len(abs_errors)*100:.1f}%)")
        print(f"   ä¸€èˆ¬ (6-10å²): {fair} ä¸ª ({fair/len(abs_errors)*100:.1f}%)")
        print(f"   è¾ƒå·® (>10å²): {poor} ä¸ª ({poor/len(abs_errors)*100:.1f}%)")
        
        print(f"\nğŸ‰ 100%çœŸå®UTKFaceæ•°æ®CSVè¡¨æ ¼ç”Ÿæˆå®Œæˆï¼")
        print(f"ğŸ“ æ–‡ä»¶: {output_path}")
        print(f"ğŸ“‹ æ ¼å¼: 30ç»´çœŸå®ç‰¹å¾ | é¢„æµ‹å¹´é¾„ | çœŸå®å¹´é¾„ | ç»å¯¹è¯¯å·®")
        print(f"âœ¨ ç‰¹ç‚¹: å®Œå…¨åŸºäºçœŸå®UTKFaceå›¾åƒæ•°æ®ï¼Œæ— ä»»ä½•æ¨¡æ‹Ÿæˆåˆ†")
        
        return str(output_path)
        
    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {str(e)}")
        print(f"\nğŸ’¡ è§£å†³æ–¹æ¡ˆ:")
        print(f"   1. è¯·ç¡®ä¿å·²ä¸‹è½½çœŸå®UTKFaceæ•°æ®é›†åˆ° data/ ç›®å½•")
        print(f"   2. å›¾åƒæ–‡ä»¶åº”ç¬¦åˆUTKFaceæ ¼å¼: [age]_[gender]_[race]_[timestamp].jpg")
        print(f"   3. è‡³å°‘éœ€è¦50ä¸ªæœ‰æ•ˆçš„çœŸå®å›¾åƒæ–‡ä»¶")
        print(f"   4. å¯ä»ä»¥ä¸‹ç½‘ç«™æ‰‹åŠ¨ä¸‹è½½:")
        print(f"      - https://susanqq.github.io/UTKFace/")
        print(f"      - https://www.kaggle.com/datasets/jangedoo/utkface-new")
        
        import traceback
        traceback.print_exc()
        return None

if __name__ == "__main__":
    main() 