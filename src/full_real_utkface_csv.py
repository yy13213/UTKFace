#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä½¿ç”¨å…¨éƒ¨23,705ä¸ªçœŸå®UTKFaceæ•°æ®ç”Ÿæˆå®Œæ•´CSVè¡¨æ ¼
100%çœŸå®æ•°æ®ï¼Œç»æ— æ¨¡æ‹Ÿæˆ–ç”Ÿæˆæ•°æ®
"""

import os
import sys
import numpy as np
import pandas as pd
from pathlib import Path
import cv2
from PIL import Image
from typing import List, Tuple, Optional, Dict
import re
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import torch
import torchvision.transforms as transforms
import time
from datetime import datetime

class FullRealUTKFaceProcessor:
    """å¤„ç†å…¨éƒ¨çœŸå®UTKFaceæ•°æ®çš„å®Œæ•´å¤„ç†å™¨"""
    
    def __init__(self, data_dir: str = "data"):
        self.data_dir = Path(data_dir)
        
        # å›¾åƒé¢„å¤„ç†
        self.transform = transforms.Compose([
            transforms.Resize((128, 128)),
            transforms.ToTensor(),
        ])
        
        print(f"ğŸ¯ å…¨é‡çœŸå®UTKFaceæ•°æ®å¤„ç†å™¨åˆå§‹åŒ–")
        print(f"ğŸ“ æ•°æ®ç›®å½•: {self.data_dir.absolute()}")
    
    def parse_utkface_info(self, filename: str) -> Optional[dict]:
        """è§£æUTKFaceæ–‡ä»¶åæ ¼å¼: [age]_[gender]_[race]_[timestamp].jpg"""
        try:
            # ç§»é™¤æ‰©å±•å
            base_name = Path(filename).stem
            
            # UTKFaceæ ¼å¼: age_gender_race_timestamp
            parts = base_name.split('_')
            if len(parts) >= 4:
                age = int(parts[0])
                gender = int(parts[1])  # 0=female, 1=male
                race = int(parts[2])    # 0=White, 1=Black, 2=Asian, 3=Indian, 4=Others
                timestamp = parts[3]
                
                # éªŒè¯æ•°æ®åˆç†æ€§
                if 0 <= age <= 120 and 0 <= gender <= 1 and 0 <= race <= 4:
                    return {
                        'age': age,
                        'gender': gender,
                        'race': race,
                        'timestamp': timestamp
                    }
            return None
            
        except (ValueError, IndexError) as e:
            return None
    
    def extract_facial_features(self, image_path: Path) -> Optional[np.ndarray]:
        """ä»çœŸå®é¢éƒ¨å›¾åƒä¸­æå–30ç»´ç‰¹å¾"""
        try:
            # åŠ è½½å›¾åƒ
            image = Image.open(image_path).convert('RGB')
            tensor = self.transform(image)
            img_array = tensor.numpy()  # shape: (3, 128, 128)
            
            features = []
            
            # RGBé€šé“ç»Ÿè®¡ç‰¹å¾ (21ç»´)
            for channel in range(3):
                channel_data = img_array[channel].flatten()
                
                # 7ä¸ªç»Ÿè®¡ç‰¹å¾
                channel_features = [
                    np.mean(channel_data),      # å‡å€¼
                    np.std(channel_data),       # æ ‡å‡†å·®
                    np.median(channel_data),    # ä¸­ä½æ•°
                    np.percentile(channel_data, 25),  # 25%åˆ†ä½æ•°
                    np.percentile(channel_data, 75),  # 75%åˆ†ä½æ•°
                    np.min(channel_data),       # æœ€å°å€¼
                    np.max(channel_data),       # æœ€å¤§å€¼
                ]
                features.extend(channel_features)
            
            # å…¨å±€ç»Ÿè®¡ç‰¹å¾ (5ç»´)
            all_pixels = img_array.flatten()
            global_features = [
                np.mean(all_pixels),                    # å…¨å±€å‡å€¼
                np.std(all_pixels),                     # å…¨å±€æ ‡å‡†å·®
                np.var(all_pixels),                     # å…¨å±€æ–¹å·®
                np.sum(all_pixels > np.mean(all_pixels)), # äº®åƒç´ æ•°
                np.sum(all_pixels < np.mean(all_pixels)), # æš—åƒç´ æ•°
            ]
            features.extend(global_features)
            
            # çº¹ç†ç‰¹å¾ (4ç»´) - åŸºäºæ¢¯åº¦
            gray = np.mean(img_array, axis=0)  # è½¬ä¸ºç°åº¦
            
            # è®¡ç®—å›¾åƒæ¢¯åº¦
            grad_x = np.diff(gray, axis=1)
            grad_y = np.diff(gray, axis=0)
            
            texture_features = [
                np.mean(np.abs(grad_x)),    # Xæ–¹å‘æ¢¯åº¦å‡å€¼
                np.mean(np.abs(grad_y)),    # Yæ–¹å‘æ¢¯åº¦å‡å€¼  
                np.std(grad_x),             # Xæ–¹å‘æ¢¯åº¦æ ‡å‡†å·®
                np.std(grad_y),             # Yæ–¹å‘æ¢¯åº¦æ ‡å‡†å·®
            ]
            features.extend(texture_features)
            
            return np.array(features)
            
        except Exception as e:
            return None
    
    def find_all_utkface_images(self) -> List[Path]:
        """æ‰¾åˆ°æ‰€æœ‰çœŸå®UTKFaceå›¾åƒ"""
        print(f"ğŸ” æœç´¢æ‰€æœ‰çœŸå®UTKFaceå›¾åƒ...")
        
        # å¤šç§å¯èƒ½çš„æœç´¢è·¯å¾„
        search_patterns = [
            self.data_dir / "*.jpg",
            self.data_dir / "*.jpeg",
            self.data_dir / "**/*.jpg", 
            self.data_dir / "**/*.jpeg",
        ]
        
        all_images = set()
        for pattern in search_patterns:
            try:
                images = list(Path().glob(str(pattern)))
                all_images.update(images)
            except:
                continue
        
        print(f"   å‘ç° {len(all_images)} ä¸ªå›¾åƒæ–‡ä»¶")
        
        # éªŒè¯UTKFaceæ ¼å¼
        valid_images = []
        invalid_count = 0
        
        for img_path in all_images:
            info = self.parse_utkface_info(img_path.name)
            if info and 0 <= info['age'] <= 120:
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”å¯è¯»
                if img_path.exists() and img_path.stat().st_size > 0:
                    valid_images.append(img_path)
                else:
                    invalid_count += 1
            else:
                invalid_count += 1
        
        print(f"   âœ… æœ‰æ•ˆUTKFaceæ–‡ä»¶: {len(valid_images)}")
        print(f"   âŒ æ— æ•ˆæ–‡ä»¶: {invalid_count}")
        
        return valid_images
    
    def load_full_dataset(self) -> Tuple[np.ndarray, np.ndarray, List[str], List[dict]]:
        """åŠ è½½å…¨éƒ¨çœŸå®UTKFaceæ•°æ®é›† - æ— æ ·æœ¬æ•°é‡é™åˆ¶"""
        print(f"ğŸ“‚ åŠ è½½å…¨éƒ¨çœŸå®UTKFaceæ•°æ®é›†...")
        
        # æ‰¾åˆ°æ‰€æœ‰å›¾åƒ
        all_images = self.find_all_utkface_images()
        
        if len(all_images) == 0:
            raise ValueError("âŒ æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„çœŸå®UTKFaceå›¾åƒæ–‡ä»¶!")
        
        print(f"   ğŸ¯ å‡†å¤‡å¤„ç† {len(all_images)} ä¸ªçœŸå®å›¾åƒ (100%çœŸå®æ•°æ®)")
        
        # æå–ç‰¹å¾å’Œæ ‡ç­¾
        features_list = []
        ages_list = []
        filenames_list = []
        info_list = []
        
        print(f"   ğŸ”„ æ­£åœ¨ä»çœŸå®å›¾åƒæå–ç‰¹å¾...")
        processed = 0
        failed = 0
        start_time = time.time()
        
        # æŒ‰æ‰¹æ¬¡å¤„ç†ä»¥å‡å°‘å†…å­˜å ç”¨
        batch_size = 100
        
        for i, img_path in enumerate(all_images):
            # è§£ææ–‡ä»¶ä¿¡æ¯
            info = self.parse_utkface_info(img_path.name)
            if not info:
                failed += 1
                continue
            
            # æå–ç‰¹å¾
            features = self.extract_facial_features(img_path)
            
            if features is not None:
                features_list.append(features)
                ages_list.append(info['age'])
                filenames_list.append(img_path.name)
                info_list.append(info)
                processed += 1
            else:
                failed += 1
            
            # è¿›åº¦æŠ¥å‘Š
            if (processed + failed) % 500 == 0 or (i + 1) == len(all_images):
                elapsed = time.time() - start_time
                rate = (processed + failed) / elapsed if elapsed > 0 else 0
                eta = (len(all_images) - (processed + failed)) / rate if rate > 0 else 0
                
                print(f"   ğŸ“Š è¿›åº¦: {processed + failed}/{len(all_images)} "
                      f"(æˆåŠŸ: {processed}, å¤±è´¥: {failed}) "
                      f"[{rate:.1f} ä¸ª/ç§’, ETA: {eta/60:.1f}åˆ†é’Ÿ]")
        
        if len(features_list) == 0:
            raise ValueError("âŒ æ— æ³•ä»ä»»ä½•å›¾åƒä¸­æå–æœ‰æ•ˆç‰¹å¾!")
        
        print(f"   âœ… æˆåŠŸå¤„ç† {len(features_list)} ä¸ªçœŸå®æ ·æœ¬")
        print(f"   â±ï¸  æ€»è€—æ—¶: {(time.time() - start_time)/60:.1f} åˆ†é’Ÿ")
        
        # ç»Ÿè®¡åˆ†æ
        ages_array = np.array(ages_list)
        genders = [info['gender'] for info in info_list]
        races = [info['race'] for info in info_list]
        
        print(f"\nğŸ“Š å®Œæ•´æ•°æ®é›†ç»Ÿè®¡:")
        print(f"   ğŸ“ˆ æ€»æ ·æœ¬æ•°: {len(features_list)} (100%çœŸå®)")
        print(f"   ğŸ“ˆ å¹´é¾„èŒƒå›´: {min(ages_list)}-{max(ages_list)} å²")
        print(f"   ğŸ“ˆ å¹³å‡å¹´é¾„: {np.mean(ages_list):.1f} å²")
        print(f"   ğŸ“ˆ å¹´é¾„åˆ†å¸ƒ:")
        print(f"      0-20å²: {np.sum((ages_array >= 0) & (ages_array <= 20))} ä¸ª")
        print(f"      21-40å²: {np.sum((ages_array >= 21) & (ages_array <= 40))} ä¸ª")
        print(f"      41-60å²: {np.sum((ages_array >= 41) & (ages_array <= 60))} ä¸ª")
        print(f"      61+å²: {np.sum(ages_array > 60)} ä¸ª")
        print(f"   ğŸ“ˆ æ€§åˆ«åˆ†å¸ƒ: å¥³æ€§ {genders.count(0)}, ç”·æ€§ {genders.count(1)}")
        print(f"   ğŸ“ˆ ç§æ—åˆ†å¸ƒ: ç™½äºº {races.count(0)}, é»‘äºº {races.count(1)}, äºšæ´²äºº {races.count(2)}, å°åº¦äºº {races.count(3)}, å…¶ä»– {races.count(4)}")
        
        return np.array(features_list), np.array(ages_list), filenames_list, info_list

def create_full_real_utkface_csv() -> str:
    """åˆ›å»ºåŸºäºå…¨éƒ¨çœŸå®UTKFaceæ•°æ®çš„å®Œæ•´CSVè¡¨æ ¼"""
    
    print("ğŸ¯ å…¨é‡çœŸå®UTKFaceæ•°æ®CSVè¡¨æ ¼ç”Ÿæˆå™¨")
    print("=" * 80)
    print("ğŸ“‹ å¤„ç†ç›®æ ‡:")
    print("   ğŸ¯ ä½¿ç”¨å…¨éƒ¨23,705ä¸ªçœŸå®UTKFaceå›¾åƒ")
    print("   âœ… 100%ä½¿ç”¨çœŸå®æ•°æ®ï¼Œç»æ— æ¨¡æ‹Ÿæ•°æ®")
    print("   âœ… ä»çœŸå®äººè„¸å›¾åƒæå–30ç»´ç‰¹å¾")
    print("   âœ… ä½¿ç”¨å›¾åƒæ–‡ä»¶åä¸­çš„çœŸå®å¹´é¾„æ ‡ç­¾")
    print("   ğŸ“Š ç”Ÿæˆå®Œæ•´çš„æµ‹è¯•ç»“æœCSVè¡¨æ ¼")
    print("=" * 80)
    
    # 1. æ£€æŸ¥æ•°æ®ç›®å½•
    data_dir = Path("data")
    if not data_dir.exists():
        print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir.absolute()}")
        return ""
    
    # 2. åˆå§‹åŒ–å¤„ç†å™¨
    processor = FullRealUTKFaceProcessor("data")
    
    # 3. åŠ è½½å…¨é‡çœŸå®æ•°æ®é›†
    print(f"\nğŸš€ å¼€å§‹åŠ è½½å…¨é‡çœŸå®æ•°æ®é›†...")
    features, ages, filenames, info_list = processor.load_full_dataset()
    
    print(f"\nğŸ“Š å…¨é‡çœŸå®æ•°æ®é›†åŠ è½½å®Œæˆ:")
    print(f"   æ ·æœ¬æ•°é‡: {len(features)} (ç›®æ ‡: 23,705)")
    print(f"   ç‰¹å¾ç»´åº¦: {features.shape[1]} (30ç»´)")
    print(f"   æ•°æ®æ¥æº: 100%çœŸå®UTKFaceå›¾åƒ")
    
    # 4. ç‰¹å¾æ ‡å‡†åŒ–
    print(f"\nğŸ”§ æ•°æ®é¢„å¤„ç†...")
    scaler = StandardScaler()
    features_scaled = scaler.fit_transform(features)
    
    # 5. æ•°æ®åˆ’åˆ† - ä½¿ç”¨æ›´å¤§çš„æµ‹è¯•é›†
    test_size = 0.3  # ä½¿ç”¨30%ä½œä¸ºæµ‹è¯•é›†ï¼Œç¡®ä¿æœ‰è¶³å¤Ÿå¤šçš„æµ‹è¯•æ ·æœ¬
    X_train, X_test, y_train, y_test, files_train, files_test = train_test_split(
        features_scaled, ages, filenames, test_size=test_size, random_state=42, stratify=None
    )
    
    print(f"   è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬ (çœŸå®)")
    print(f"   æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬ (çœŸå®)")
    
    # 6. è®­ç»ƒå¹´é¾„é¢„æµ‹æ¨¡å‹
    print(f"\nğŸ¯ è®­ç»ƒå¹´é¾„é¢„æµ‹æ¨¡å‹...")
    model = RandomForestRegressor(
        n_estimators=500,      # å¢åŠ æ ‘çš„æ•°é‡
        max_depth=25,          # å¢åŠ æ·±åº¦
        random_state=42,
        min_samples_split=2,
        min_samples_leaf=1,
        max_features='sqrt',
        n_jobs=-1,
        verbose=1
    )
    
    print(f"   ğŸ”„ æ­£åœ¨è®­ç»ƒæ¨¡å‹...")
    start_time = time.time()
    model.fit(X_train, y_train)
    training_time = time.time() - start_time
    print(f"   âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ (è€—æ—¶: {training_time:.1f}ç§’)")
    
    # 7. è¿›è¡Œé¢„æµ‹
    print(f"\nğŸ”® è¿›è¡Œå¹´é¾„é¢„æµ‹...")
    y_pred_test = model.predict(X_test)
    y_pred_train = model.predict(X_train)  # å¯¹è®­ç»ƒé›†ä¹Ÿè¿›è¡Œé¢„æµ‹
    
    # 8. è®¡ç®—è¯¯å·®
    abs_errors_test = np.abs(y_test - y_pred_test)
    abs_errors_train = np.abs(y_train - y_pred_train)  # è®­ç»ƒé›†è¯¯å·®
    
    # 9. åˆ›å»ºç»“æœDataFrame - åˆå¹¶è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    print(f"\nğŸ“Š ç”Ÿæˆå®Œæ•´CSVè¡¨æ ¼...")
    
    # ç‰¹å¾åˆ—å
    feature_names = []
    # RGBç‰¹å¾ (21ç»´)
    for channel in ['R', 'G', 'B']:
        for stat in ['mean', 'std', 'median', 'q25', 'q75', 'min', 'max']:
            feature_names.append(f'{channel}_{stat}')
    
    # å…¨å±€ç‰¹å¾ (5ç»´)
    global_names = ['global_mean', 'global_std', 'global_var', 'bright_pixels', 'dark_pixels']
    feature_names.extend(global_names)
    
    # çº¹ç†ç‰¹å¾ (4ç»´)
    texture_names = ['grad_x_mean', 'grad_y_mean', 'grad_x_std', 'grad_y_std']
    feature_names.extend(texture_names)
    
    # åˆ›å»ºè®­ç»ƒé›†æ•°æ®æ¡†
    train_df = pd.DataFrame(X_train, columns=feature_names)
    train_df['Predicted_Age'] = y_pred_train
    train_df['Actual_Age'] = y_train
    train_df['Abs_Error'] = abs_errors_train
    train_df['Data_Type'] = 'Train'
    
    # åˆ›å»ºæµ‹è¯•é›†æ•°æ®æ¡†
    test_df = pd.DataFrame(X_test, columns=feature_names)
    test_df['Predicted_Age'] = y_pred_test
    test_df['Actual_Age'] = y_test
    test_df['Abs_Error'] = abs_errors_test
    test_df['Data_Type'] = 'Test'
    
    # åˆå¹¶è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    complete_df = pd.concat([train_df, test_df], ignore_index=True)
    
    # æŒ‰ç»å¯¹è¯¯å·®æ’åº
    complete_df = complete_df.sort_values('Abs_Error')
    
    print(f"   âœ… å®Œæ•´æ•°æ®æ¡†åˆ›å»ºå®Œæˆ: {len(complete_df)} è¡Œ")
    print(f"      - è®­ç»ƒé›†æ ·æœ¬: {len(train_df)} ä¸ª")
    print(f"      - æµ‹è¯•é›†æ ·æœ¬: {len(test_df)} ä¸ª")
    
    # 10. ä¿å­˜å®Œæ•´CSVæ–‡ä»¶
    output_dir = Path("results/metrics")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    csv_filename = f"full_real_utkface_complete_{len(complete_df)}samples_{timestamp}.csv"
    csv_path = output_dir / csv_filename
    
    complete_df.to_csv(csv_path, index=False)
    
    # 11. æ€§èƒ½è¯„ä¼° - åˆ†åˆ«è¯„ä¼°è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    # æµ‹è¯•é›†æ€§èƒ½
    mae_test = mean_absolute_error(y_test, y_pred_test)
    mse_test = mean_squared_error(y_test, y_pred_test)
    rmse_test = np.sqrt(mse_test)
    r2_test = r2_score(y_test, y_pred_test)
    
    # è®­ç»ƒé›†æ€§èƒ½
    mae_train = mean_absolute_error(y_train, y_pred_train)
    mse_train = mean_squared_error(y_train, y_pred_train)
    rmse_train = np.sqrt(mse_train)
    r2_train = r2_score(y_train, y_pred_train)
    
    # æ•´ä½“æ€§èƒ½
    y_all = np.concatenate([y_train, y_test])
    y_pred_all = np.concatenate([y_pred_train, y_pred_test])
    abs_errors_all = np.concatenate([abs_errors_train, abs_errors_test])
    
    mae_all = mean_absolute_error(y_all, y_pred_all)
    rmse_all = np.sqrt(mean_squared_error(y_all, y_pred_all))
    r2_all = r2_score(y_all, y_pred_all)
    
    print(f"\nğŸ“ˆ å®Œæ•´æ¨¡å‹æ€§èƒ½è¯„ä¼°:")
    print(f"\nğŸ¯ æ•´ä½“æ€§èƒ½ (å…¨éƒ¨{len(complete_df)}ä¸ªæ ·æœ¬):")
    print(f"   å¹³å‡ç»å¯¹è¯¯å·® (MAE): {mae_all:.2f} å¹´")
    print(f"   å‡æ–¹æ ¹è¯¯å·® (RMSE): {rmse_all:.2f} å¹´")
    print(f"   RÂ² å†³å®šç³»æ•°: {r2_all:.3f}")
    print(f"   è¯¯å·®æ ‡å‡†å·®: {np.std(abs_errors_all):.2f} å¹´")
    print(f"   æœ€å°è¯¯å·®: {np.min(abs_errors_all):.2f} å¹´")
    print(f"   æœ€å¤§è¯¯å·®: {np.max(abs_errors_all):.2f} å¹´")
    print(f"   ä¸­ä½æ•°è¯¯å·®: {np.median(abs_errors_all):.2f} å¹´")
    
    print(f"\nğŸ“Š è®­ç»ƒé›†æ€§èƒ½ ({len(train_df)}ä¸ªæ ·æœ¬):")
    print(f"   å¹³å‡ç»å¯¹è¯¯å·® (MAE): {mae_train:.2f} å¹´")
    print(f"   å‡æ–¹æ ¹è¯¯å·® (RMSE): {rmse_train:.2f} å¹´")
    print(f"   RÂ² å†³å®šç³»æ•°: {r2_train:.3f}")
    
    print(f"\nğŸ”¬ æµ‹è¯•é›†æ€§èƒ½ ({len(test_df)}ä¸ªæ ·æœ¬):")
    print(f"   å¹³å‡ç»å¯¹è¯¯å·® (MAE): {mae_test:.2f} å¹´")
    print(f"   å‡æ–¹æ ¹è¯¯å·® (RMSE): {rmse_test:.2f} å¹´")
    print(f"   RÂ² å†³å®šç³»æ•°: {r2_test:.3f}")
    
    # 12. ä¿å­˜è¯¦ç»†æ€§èƒ½æ‘˜è¦
    summary_data = {
        'Metric': ['Total_Samples', 'Train_Samples', 'Test_Samples', 
                  'Overall_MAE', 'Overall_RMSE', 'Overall_R2',
                  'Train_MAE', 'Train_RMSE', 'Train_R2',
                  'Test_MAE', 'Test_RMSE', 'Test_R2',
                  'Overall_Error_Std', 'Overall_Min_Error', 'Overall_Max_Error', 'Overall_Median_Error'],
        'Value': [len(features), len(X_train), len(X_test),
                 mae_all, rmse_all, r2_all,
                 mae_train, rmse_train, r2_train,
                 mae_test, rmse_test, r2_test,
                 np.std(abs_errors_all), np.min(abs_errors_all), 
                 np.max(abs_errors_all), np.median(abs_errors_all)],
        'Description': ['æ€»æ ·æœ¬æ•°', 'è®­ç»ƒæ ·æœ¬æ•°', 'æµ‹è¯•æ ·æœ¬æ•°',
                       'æ•´ä½“å¹³å‡ç»å¯¹è¯¯å·®', 'æ•´ä½“å‡æ–¹æ ¹è¯¯å·®', 'æ•´ä½“RÂ²å†³å®šç³»æ•°',
                       'è®­ç»ƒé›†å¹³å‡ç»å¯¹è¯¯å·®', 'è®­ç»ƒé›†å‡æ–¹æ ¹è¯¯å·®', 'è®­ç»ƒé›†RÂ²å†³å®šç³»æ•°',
                       'æµ‹è¯•é›†å¹³å‡ç»å¯¹è¯¯å·®', 'æµ‹è¯•é›†å‡æ–¹æ ¹è¯¯å·®', 'æµ‹è¯•é›†RÂ²å†³å®šç³»æ•°',
                       'æ•´ä½“è¯¯å·®æ ‡å‡†å·®', 'æ•´ä½“æœ€å°è¯¯å·®', 'æ•´ä½“æœ€å¤§è¯¯å·®', 'æ•´ä½“ä¸­ä½æ•°è¯¯å·®']
    }
    
    summary_df = pd.DataFrame(summary_data)
    summary_path = output_dir / f"full_real_utkface_complete_summary_{timestamp}.csv"
    summary_df.to_csv(summary_path, index=False)
    
    print(f"\nğŸ’¾ æ–‡ä»¶ä¿å­˜å®Œæˆ:")
    print(f"   ğŸ“Š å®Œæ•´ç»“æœ: {csv_path}")
    print(f"   ğŸ“‹ æ€§èƒ½æ‘˜è¦: {summary_path}")
    print(f"   ğŸ“ æ–‡ä»¶å¤§å°: {csv_path.stat().st_size / 1024 / 1024:.1f} MB")
    
    print(f"\nğŸ‰ å…¨é‡çœŸå®UTKFaceå®Œæ•´æ•°æ®CSVè¡¨æ ¼ç”Ÿæˆå®Œæˆ!")
    print(f"   âœ… ä½¿ç”¨äº† {len(features)} ä¸ªçœŸå®æ ·æœ¬")
    print(f"   âœ… ç”Ÿæˆäº† {len(complete_df)} è¡Œå®Œæ•´é¢„æµ‹ç»“æœ")
    print(f"   âœ… åŒ…å«è®­ç»ƒé›† {len(train_df)} ä¸ª + æµ‹è¯•é›† {len(test_df)} ä¸ªæ ·æœ¬")
    print(f"   âœ… æ ¼å¼: 30ç»´ç‰¹å¾ + é¢„æµ‹å¹´é¾„ + çœŸå®å¹´é¾„ + ç»å¯¹è¯¯å·® + æ•°æ®ç±»å‹")
    print(f"   âœ… æ•°æ®çœŸå®æ€§: 100%çœŸå®UTKFaceæ•°æ®")
    
    return str(csv_path)

def main():
    """ä¸»å‡½æ•°"""
    try:
        csv_path = create_full_real_utkface_csv()
        
        if csv_path:
            print(f"\nğŸŒŸ ä»»åŠ¡æˆåŠŸå®Œæˆ!")
            print(f"ğŸ“ CSVæ–‡ä»¶è·¯å¾„: {csv_path}")
        else:
            print(f"\nâŒ ä»»åŠ¡å¤±è´¥!")
            
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 